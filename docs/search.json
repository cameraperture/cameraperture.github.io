[
  {
    "objectID": "olympics.html",
    "href": "olympics.html",
    "title": "Olympics",
    "section": "",
    "text": "Data for this analysis was accessed from https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-07-27 . Data was scraped from www.sports-reference.com in May 2018 and contains historical Olympics data from Athens 1896 to Rio 2016.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-27/olympics.csv')\n\n\nggplot(olympics, aes(x=year, y=age))+\n  geom_point(aes(color=sex))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nggplot(olympics, aes(x=age))+\n  geom_histogram(aes(fill=sex))+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nolympics_nona &lt;- olympics[!is.na(olympics$age), ]\nolympics_meanage&lt;-olympics_nona%&gt;%\n  group_by(year)%&gt;%\n  summarize(avg_age=(mean(age)))\n\n\nggplot(olympics_meanage, aes(x=year, y=avg_age)) +\n  geom_point(shape=16, size=2, color=\"goldenrod4\") +\n  geom_smooth(color=\"#90EE90\", fill=\"#90EE90\", se=FALSE)+\n  theme_minimal()+\n  labs(title = \"Mean Age of Athletes at the Olympics\",\n       subtitle = \"1896 to 2016\",\n       x=\"Olympic Year\",\n       y=\"Mean Age\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Charlotte Imbert",
    "section": "",
    "text": "Welcome! My name is Charlotte Imbert and I’m an undergraduate student at Pomona College. I am majoring in Neuroscience with a minor in Math. I love Statistics and ML and am currently working on a research project aiming to predict cognitive function from neuroimaging (fMRI) data.\nWhen I’m not studying or doing research, I love to play pickleball and perform stand-up comedy. Additionally, I love Daft Punk, Taylor Swift, cheese, and visiting National Parks. I am a French and British dual citizen, and I speak fluent French, Spanish and Portuguese."
  },
  {
    "objectID": "premierleague.html",
    "href": "premierleague.html",
    "title": "Premier League Soccer",
    "section": "",
    "text": "The data for this analysis was accessed via https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-04-04 . The data is from the Premier League Match Data 2021-2022 via Evan Gower on Kaggle. The dataset contains information about soccer matches in the 2021-2022 English Premier League season.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')\n\n\nsoccer_summary&lt;-soccer%&gt;%\n  group_by(HomeTeam)%&gt;%\n  summarize(totalfouls=sum(HF))%&gt;%\n  arrange(desc(totalfouls))\n\n\nplot&lt;-ggplot(soccer_summary, aes(x=reorder(HomeTeam, totalfouls), y=totalfouls))+\n  geom_bar(stat = \"identity\", aes(fill = ifelse(HomeTeam %in% c(\"Burnley\", \"Norwich\", \"Watford\"), \"Relegated\", \"Safe\"))) +\n  scale_fill_manual(values = c(\"Relegated\" = \"navyblue\", \"Safe\" = \"lightblue\")) +\n  theme_minimal()+\n  labs(title=\"Premier League Fouls by Team, 2021-2022\",\n       subtitle=\"Home Games\",\n       y=\"Total Number of Fouls\",\n       x=\"Team\",\n       fill=\"Relegation Status\")\nplot+coord_flip()\n\n\n\n\n\n\n\n\n\nsoccer_summary2&lt;-soccer%&gt;%\n  group_by(AwayTeam)%&gt;%\n  summarize(totalfouls_a=sum(AF))%&gt;%\n  arrange(desc(totalfouls_a))\n\n\nplot2&lt;-ggplot(soccer_summary2, aes(x=reorder(AwayTeam, totalfouls_a), y=totalfouls_a))+\n  geom_bar(stat = \"identity\", aes(fill = ifelse(AwayTeam %in% c(\"Burnley\", \"Norwich\", \"Watford\"), \"Relegated\", \"Safe\"))) +\n  scale_fill_manual(values = c(\"Relegated\" = \"red\", \"Safe\" = \"lightpink\")) +\n  theme_minimal()+\n  labs(title=\"Premier League Fouls by Team, 2021-2022\",\n       subtitle=\"Away Games\",\n       y=\"Total Number of Fouls\",\n       x=\"Team\",\n       fill=\"Relegation Status\")\nplot2+coord_flip()"
  },
  {
    "objectID": "shakespeare.html",
    "href": "shakespeare.html",
    "title": "Shakespeare",
    "section": "",
    "text": "Data was accessed from https://github.com/rfordatascience/tidytuesday/tree/master/data/2024/2024-09-17 . The dataset is from shakespeare.mit.edu (via github.com/nrennie/shakespeare). Some of the code below is taken from Deepali Kank on GitHub, https://github.com/deepdk/TidyTuesday2024/tree/main/2024/week_38 .\nlibrary(tidyverse)\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/romeo_juliet.csv')\nlibrary(scales)\nlibrary(tidytext)\nlibrary(devtools)\nlibrary(ggwordcloud)\nlibrary(png)\nlibrary(svglite)"
  },
  {
    "objectID": "shakespeare.html#romeo-juliet",
    "href": "shakespeare.html#romeo-juliet",
    "title": "Shakespeare",
    "section": "Romeo & Juliet",
    "text": "Romeo & Juliet\n\nromeo_juliet&lt;-romeo_juliet%&gt;%\n  filter(character %in% c(\"Romeo\", \"Juliet\"))\n\nWe need to remove stop words:\n\ncustom_stop_words &lt;- c(\"thou\", \"thy\", \"thee\", \"thine\", \"art\", \"hast\", \"doth\", \"dost\", \"ere\", \"o\",\"hath\")\n\n\ntemp1 &lt;- romeo_juliet %&gt;%\n  unnest_tokens(word, dialogue) %&gt;%\n  anti_join(stop_words)%&gt;%\n  filter(!str_detect(word, \"^[0-9]+$\"))\n\nJoining with `by = join_by(word)`\n\ntemp2 &lt;- romeo_juliet %&gt;%\n  unnest_tokens(word, dialogue)%&gt;%\n  filter(!word %in% stop_words$word)%&gt;%\n  filter(!str_detect(word, \"^[0-9]+$\"))\n\n\nword_counts &lt;- romeo_juliet %&gt;%\n  unnest_tokens(word, dialogue) %&gt;%\n  anti_join(stop_words) %&gt;%\n  filter(!str_detect(word, \"^[0-9]+$\")) %&gt;% #remove any word, and a full number. ^ is beginning of string, + means one or more, and $ is end of string\n  filter(!word %in% custom_stop_words) %&gt;%\n  mutate(word=stringr::str_replace(word, \"'s\", \"\"))%&gt;%\n  count(character, word, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\n\n\njuliet &lt;- word_counts |&gt; \n  filter(character == \"Juliet\")\n\n\nromeo &lt;- word_counts |&gt; \n  filter(character == \"Romeo\")\n\nMaking a word cloud for each character:\n\nromeo %&gt;%\n  filter(n&gt;1)%&gt;%\n  ggplot(aes(label=word, size=n, color=n))+\n  geom_text_wordcloud(rm_outside = TRUE, shape=\"cardioid\")+\n  scale_size_area(max_size = 15) +\n  theme_minimal() +\n  scale_color_gradient(low = \"#FF69B4\", high = \"#C41E3A\")\n\nWarning in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :\nSome words could not fit on page. They have been removed.\n\n\n\n\n\n\n\n\n\n\np1_ro &lt;- ggplot(\n  romeo,\n  aes(\n    label = word, size = n,color = n\n  )\n) +\n  geom_text_wordcloud_area(\n    mask = readPNG(\"/Users/charlotteimbert/Desktop/git/cameraperture.github.io/AlphaShakespeare.png\"),\n    rm_outside = TRUE\n  ) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  scale_color_gradient(low = \"#FF69B4\", high = \"#C41E3A\")\n\nprint(p1_ro)\n\nWarning in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :\nSome words could not fit on page. They have been removed.\n\n\n\n\n\n\n\n\n\n\np1_ju &lt;- ggplot(\n  juliet,\n  aes(\n    label = word, size = n,color = n\n  )\n) +\n  geom_text_wordcloud_area(\n    mask = readPNG(\"/Users/charlotteimbert/Desktop/git/cameraperture.github.io/AlphaShakespeare.png\"),\n    rm_outside = TRUE\n  ) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  scale_color_gradient(low = \"#FF69B4\", high = \"#C41E3A\")\n\nprint(p1_ju)\n\nWarning in wordcloud_boxes(data_points = points_valid_first, boxes = boxes, :\nSome words could not fit on page. They have been removed."
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "DS002 - Mini Project 2",
    "section": "",
    "text": "The data used for this analysis are the “Dear Abby” stories from 1985 to 2017, underlying The Pudding’s “30 years of American anxieties” article from November 2018 (https://pudding.cool/2018/11/dearabby/).\nData was accessed via The Pudding’s GitHub, at https://github.com/the-pudding/data/tree/master/dearabby.\nThe dataset consists of 20034 observations, each corresponding to a question sent in to ‘Dear Abby’, a popular advice column.\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")"
  },
  {
    "objectID": "miniproject2.html#data-source",
    "href": "miniproject2.html#data-source",
    "title": "DS002 - Mini Project 2",
    "section": "",
    "text": "The data used for this analysis are the “Dear Abby” stories from 1985 to 2017, underlying The Pudding’s “30 years of American anxieties” article from November 2018 (https://pudding.cool/2018/11/dearabby/).\nData was accessed via The Pudding’s GitHub, at https://github.com/the-pudding/data/tree/master/dearabby.\nThe dataset consists of 20034 observations, each corresponding to a question sent in to ‘Dear Abby’, a popular advice column.\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")"
  },
  {
    "objectID": "miniproject2.html#eda",
    "href": "miniproject2.html#eda",
    "title": "DS002 - Mini Project 2",
    "section": "EDA",
    "text": "EDA\nIt seems that the questions in the dataset are already all in lowercase, but I will apply the str_to_lower() function to ensure my analysis is accurate.\n\ndata&lt;-data|&gt;\n  mutate(question_only=str_to_lower(question_only))\n\n\nFamily Words\nIt is reasonable to assume that a number of these questions might relate to family matters, which are often pressing but too taboo to discuss with the family members involved.\nBelow, I use regular expressions to identify the most commonly occurring family members in the questions from 1985 to 2017.\nI am considering ‘wife’, ‘husband’, ‘son’ and ‘daughter’ (and their plural and possessive forms) as these are the most immediate family members.\nI am also including ‘marriage’ and ‘divorce’ since these are likely sources of distress or conflict that could push somebody to submit a question to the column.\n\nwife&lt;-sum(grepl(\"\\\\bwives\\\\b|\\\\bwife'?s?'?\\\\b\", data$question_only))\nhusband&lt;-sum(grepl(\"\\\\bhusband'?s?'?\\\\b\", data$question_only))\nson&lt;-sum(grepl(\"\\\\bson'?s?'?\\\\b\", data$question_only))\ndaughter&lt;-sum(grepl(\"\\\\bdaughter'?s?'?\\\\b\", data$question_only))\nmarriage&lt;-sum(grepl(\"\\\\bmarriage'?s?'?\\\\b|\\\\bmarried\\\\b\", data$question_only))\ndivorce&lt;-sum(grepl(\"\\\\bdivorces?d?\", data$question_only))\nfamily_data&lt;-data.frame(Wife=wife, Husband=husband, Son=son, Daughter=daughter, Marriage=marriage, Divorce=divorce)\n\nThe table below shows the number of questions asked to Abby from 1985 to 2017 that included each term (including its plural or possessive forms).\n\nprint(family_data)\n\n  Wife Husband  Son Daughter Marriage Divorce\n1 2775    5124 2320     2750     4887    1534\n\n\nIt is very interesting to note that ‘wife’, ‘son’ and ‘daughter’ all appeared a similar number of times, while ‘husband’ was by far the most frequent, appearing almost twice as many times as the other terms (5124 times - this is approximately one quarter of the 20034 questions!) ‘Marriage’ was also very common, starring in 4887 questions.\n\n\nNumber of Questions Through the Years\nFor some more exploratory data analysis, I wanted to observe how the number of questions per year changed over time:\n\ny_questions&lt;-data|&gt; group_by(year)|&gt; summarize(n_questions=n())\n\n\nggplot(y_questions, aes(x=year, y=n_questions))+\n  geom_point(color='pink')+\n  theme_dark()+\n  labs(x=\"Year\",\n       y=\"Number of Questions\",\n       title=\"'Dear Abby' Questions Over The Years\",\n       subtitle=\"From 1985 to 2017\")\n\n\n\n\n\n\n\n\nIt seems like 1985 had an abnormally high number of questions. After this, the number of questions decreased steadily until 1999, when the number of questions per year began to increase again. It did so until 2016, and experienced a steep drop in 2017.\n\n\nQuestion Length\nThen, I was curious about the length of the questions in the dataset:\n\ndata|&gt;\n  select(year, question_only)|&gt;\n  mutate(size=str_length(question_only))|&gt;\n  arrange(desc(size))|&gt;\n  slice_head(n=10)\n\n# A tibble: 10 × 3\n    year question_only                                                      size\n   &lt;dbl&gt; &lt;chr&gt;                                                             &lt;int&gt;\n 1  1988 \"media-wise, this is going to be a tough campaign for barbara bu… 13889\n 2  1995 \"on one page of prison life magazine, an inmate describes his fa… 13411\n 3  1990 \"this story has been entered on the data base in 2 parts. this i… 13225\n 4  1988 \"kitty carlisle hart has spent her life in the\\nspotlight. panel… 11690\n 5  1992 \"this story has been entered on the data base in 3 parts. this i… 11550\n 6  1995 \"arthur donald delacy would have--by all rights, should have--tu… 11087\n 7  1989 \"with his arms stuffed in his pockets to the elbow, the cartooni… 10716\n 8  1995 \"while charles schulz sits at his table, pen in hand, drawing th… 10275\n 9  1994 \"i am so fed up with store employees treating me and my friend l…  8867\n10  1986 \"denise salvaggio's dream is to do for bugs what walt disney did…  8514\n\n\nThe longest question occurred in 1998, with a size of 13889 characters - and it was about Barbara Bush!\nWhat about the shortest question?\n\nq_length&lt;-data|&gt;\n  select(year, question_only)|&gt;\n  mutate(size=str_length(question_only))|&gt;\n  arrange(size)|&gt;\n  slice_head(n=10)\nprint(q_length)\n\n# A tibble: 10 × 3\n    year question_only                                            size\n   &lt;dbl&gt; &lt;chr&gt;                                                   &lt;int&gt;\n 1  1996 \"q: were you alone or by yourself?\"                        33\n 2  1996 \"q: so you were gone until you returned?\"                  39\n 3  1990 \"how would you define old age? getting there\"              43\n 4  1996 \"q: how long have you been a french canadian?\"             44\n 5  1986 \"what would you give a man who has everything? freda\"      51\n 6  1990 \"how would you define success? philosophy major, ucla\"     52\n 7  1996 \"q: the youngest son, the 20-year-old, how old is he?\"     52\n 8  2008 \"it's apathy ... but, who cares? -- malcolm in miami\\n\"    52\n 9  1996 \"q: do you have any children or anything of that kind?\"    53\n10  2010 \"how do you mend a broken heart? -- tears on my pillow\"    53\n\n\nIt occurred in 1996, with the rather ominous: “q: were you alone or by yourself?”. In 2010, someone asked a very hard question, but kept it incredibly succint at 53 characters: “how do you mend a broken heart?– tears on my pillow”.\n\n\n47 - Chirp Chirp!\nAt Pomona College, the number 47 holds special meaning: it is said to be a number that frequently occurs in natural settings. With this in mind, I wanted to determine how often the number 47 appears in this dataset.\n\ndata&lt;-data|&gt;\n  mutate(chirp=str_detect(question_only, \"47\"))\nsum(data$chirp)\n\n[1] 227\n\n\nThe number 47 appears 227 times in the dataset, out of 20034 letters written to Abby."
  },
  {
    "objectID": "miniproject2.html#distinguishing-common-themes-using-word-count",
    "href": "miniproject2.html#distinguishing-common-themes-using-word-count",
    "title": "DS002 - Mini Project 2",
    "section": "Distinguishing Common Themes Using Word Count",
    "text": "Distinguishing Common Themes Using Word Count\nI would like to determine which words are the most commonly occurring across all the questions to see if any common themes can be extracted.\nFirst, stop words need to be removed - these are common words in the English language that don’t have much meaning. The stop_words dataframe from the tidytext library contains 3 lexicons of stop words.\nWe should also remove the words ‘dear’ and ‘abby’ (and its variants) since those will be at the start of almost every letter.\n\ndear_abby &lt;- c(\"dear\", \"abby\", \"abby's\", \"abbys\")\n\n\nWord Cloud - What Concerned Americans Most Between 1985 and 2017?\nBelow is a word cloud displayed the 150 most common words from 32 years ‘Dear Abby’ letters (excluding stop words):\n\nwc &lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^\\\\D+$\"))|&gt;\n  filter(!word %in% dear_abby)|&gt;\n  group_by(word)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(desc(freq))\n\n\nlibrary(wordcloud2)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nwordcloud(words = wc$word, freq = wc$freq, min.freq = 600,           max.words=150, random.order=FALSE, rot.per=0, colors=brewer.pal(6, \"Set2\"), scale = c(5, 1))\n\n\n\n\n\n\n\n\nWe can see from both this word cloud and the earlier analysis done on family words that Americans are very concerned by family matters and family members: ‘husband’, ‘children’, ‘kids’, ‘mom’, ‘mother’, ‘married’, ‘family’, ‘wife’, ‘son’, ‘father’, ‘parents’, ‘sister’, to name a few.\nFrom the word cloud, we can also distinguish the most common emotions: ‘hurt’, ‘afraid’, ‘enjoy’, ‘concerned’, ‘upset’.\n\n\nMost Common Word by Year\n\nwords_by_year&lt;-data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^\\\\D+$\"))|&gt;\n  filter(!word %in% dear_abby)|&gt;\n  group_by(word, year)|&gt;\n  summarize(freq=n())|&gt;\n  ungroup()|&gt;\n  group_by(year)|&gt;\n  slice_max(n = 1, order_by=freq)|&gt;\n  arrange(year)\n\n\nggplot(words_by_year, aes(x=year, y=freq))+\n  geom_point(aes(shape=word, color=word, size=3))+\n  labs(title=\"Most Common Word in 'Dear Abby' Letters\",\n       subtitle= \"By Year, 1985 to 2017\",\n       x=\"Year\",\n       y=\"Frequency\")+\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHusband Letters Over Time\nSince ‘husband’ was the most common word between 1985 and 2017, it would be interesting to see how its frequency changed over time:\n\nhusband_year&lt;-data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^husband'?s?$\"))|&gt;\n  group_by(year)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(year)\n\n\nggplot(husband_year, aes(x=year, y=freq))+\n  geom_point(color=\"darkgreen\", shape=18, size=2.5)+\n  theme_minimal()+\n  labs(title=\"Frequency of 'Husband' in 'Dear Abby' Letters\",\n       subtitle= \"1985 to 2017\",\n       x=\"Year\",\n       y=\"Number of Occurrences\")\n\n\n\n\n\n\n\n\n\n\nElection Letters Over Time\nLastly, I was interested in determining whether the word ‘election’ (and its plural form) was a common concern. I also wanted to see if it became more frequent during US Presidential Election years.\n\nelections_year&lt;-data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^elections?$\"))|&gt;\n  group_by(year)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(year)\n\n\nggplot(elections_year, aes(x=year, y=freq))+\n  geom_bar(stat=\"identity\", aes(fill = ifelse(year %in% c(1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016), \"Yes\", \"No\")))+\n  scale_fill_manual(values = c(\"Yes\" = \"navyblue\", \"No\" = \"gray\")) +\n  theme_minimal()+\n  labs(title=\"'Dear Abby' Election Letters\",\n       subtitle=\"Years With and Without US Presidential Elections\",\n       y=\"Year\",\n       x=\"Frequency\",\n       fill=\"Election Year?\")\n\n\n\n\n\n\n\n\nThe word ‘election’ and its plural form ‘elections’ does not seem to be a popular concern, never appearing more than 4 times in one year. However, the plot above shows that these words were more common during US Presidential Election years than during years without a Presidential Election, with the exception of 1994."
  }
]