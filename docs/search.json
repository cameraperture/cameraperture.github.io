[
  {
    "objectID": "premierleague.html",
    "href": "premierleague.html",
    "title": "Premier League Soccer",
    "section": "",
    "text": "The dataset for this analysis was accessed via https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-04-04 . The data is from the Premier League Match Data 2021-2022 via Evan Gower on Kaggle. The dataset contains information about soccer matches in the 2021-2022 English Premier League season, including the date, referee, home and away team, full-time home and away goals, home and away fouls, in addition other information for each match.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Charlotte Imbert",
    "section": "",
    "text": "Welcome! My name is Charlotte Imbert and I’m a senior at Pomona College. I am majoring in Neuroscience with a minor in Math. I love Statistics and ML and am currently working on a research project aiming to predict cognitive function from neuroimaging (fMRI) data.\nWhen I’m not studying or doing research, I love to play pickleball and perform stand-up comedy. Additionally, I love Daft Punk, Taylor Swift, and visiting National Parks. I am a French and British dual citizen, and I speak fluent French, Spanish and Portuguese."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "SQL",
    "section": "",
    "text": "For this project, I will be using SQL to query Smith College’s Wideband Acoustic Immittance (WAI) Database, which contains WAI ear measurements from a multitude of scientific publications. The WAI Database is available at https://www.science.smith.edu/wai-database/. My goal is twofold. First, I aim to use SQL and ggplot to replicate Figure 1 from Susan Voss’ 2019 study, “An online wideband acoustic immittance (WAI) database and corresponding website” (Voss 2019). Second, I aim to produce a plot showing race differences in frequency versus mean absorption for one specific study in the WAI database (done in 2010 study by Voss et al.).\n\n\n\nlibrary(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nThis displays all of the tables that are in the WAI database. Measurements and Subjects will be of particular interest for what I aim to do.\nMeasurements contains information about each specific study, notably the frequency and absorbance.\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nSubjects contains information about the participants in each study, including their age, sex, race and ethnicity.\n\nSELECT *\nFROM Subjects\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nAbur_2014\n1\n7\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n3\n8\n19\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 5 not included do to acoustic leak\n\n\nAbur_2014\n4\n7\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n6\n8\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n7\n5\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\n\n\n\nPI_Info contains information about the authors, year, journal and title of each study in the database. This will be useful for including the primary author and date of publication in the replicated figure legend.\n\n\n\n\nSELECT p.Identifier, p.Year, p.AuthorsShortList, Frequency,\nLOG10(Frequency) AS log_frequency, AVG(Absorbance) AS mean_absorbance,\nCOUNT(DISTINCT SubjectNumber, Ear) AS ear_u,\nCONCAT(AuthorsShortList, \" (\" , year, \") \", \"N=\", COUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS legend\nFROM PI_Info AS p\nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier\nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\") AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument, Frequency;\n\n\nplot |&gt;\nggplot(aes(x = Frequency, y = mean_absorbance,\n  color = legend,\n  group = legend)) +\n  geom_line() +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Mean absorbance from each publication in WAI database\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Study, No of individual ears, Equipment\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n  theme(legend.text = element_text(size = 4),\n        legend.title = element_text(size = 6),\n        legend.position = c(0.01, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.4, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nThis plot above is a replication of the figure in Voss’ 2019 study (Voss 2019). It displays frequency versus mean absorbance measurements for the 12 studies included in the WAI database (as of July 2019). The authors, number of unique ears and equipment used for each study is indicated in the legend in the top left.\n\n\n\n\nSELECT s.Race, m.Frequency, AVG(m.Absorbance) AS mean_absorbance \nFROM Subjects AS s \nRIGHT JOIN Measurements AS m ON s.SubjectNumber = m.SubjectNumber\nWHERE m.Identifier = \"Voss_2010\" AND m.Frequency &gt; 200 AND m.Frequency &lt; 8000\nGROUP BY s.Race, m.Frequency;\n\n\nhead(groups_graph)\n\n              Race Frequency mean_absorbance\n1 African American  210.9375      0.06189557\n2 African American  234.3750      0.03557816\n3 African American  257.8125      0.06311273\n4 African American  281.2500      0.05740714\n5 African American  304.6875      0.06617098\n6 African American  328.1250      0.07566153\n\n\n\ngroups_graph |&gt;\n  ggplot(aes(x = Frequency, y = mean_absorbance)) +\n           geom_line(aes(color = Race)) +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Differences in Mean Absorbance by Race\",\n       subtitle = \"Voss et al., 2010\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Race\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n    theme(legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8),\n        legend.position = c(0.05, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.6, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nI chose to look at differences in mean absorbance for each frequency band by race in one specific study conducted by Voss et al. in 2010. I chose this study because of its large number of participants (1984), which should help to make trends in the data more clear. Each line represents a different race included in the study. It can be seen from the graph that mean absorbance follows very similar trends for White, Caucasian and Asian participants. Meanwhile, the mean absorbances for African American, Black and Mixed participants are noticeably different from those of the other races. Notably, African American participants tend to have a higher mean absorbance for the same frequencies than the other races included in the study, especially for frequencies between 1000 Hz and 5000 Hz. Mixed participants tend to have lower mean absorbances than the other races in the study for frequencies between 2000 Hz and 4000 Hz.\n\n\n\nIn this project I used SQL queries to obtain the necessary information from the relevant tables in the WAI database (Subjects, Measurements and PI_Info). These SQL queries enabled me to save the data as R objects, from which I could produce graphs using ggplot. I was able to replicate Figure 1 from Voss’ 2019 study (Voss 2019), and I also produced a graph that showed clear race differences in mean absorbance for Voss et al.’s 2010 study."
  },
  {
    "objectID": "project4.html#data-familiarization",
    "href": "project4.html#data-familiarization",
    "title": "SQL",
    "section": "",
    "text": "library(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nThis displays all of the tables that are in the WAI database. Measurements and Subjects will be of particular interest for what I aim to do.\nMeasurements contains information about each specific study, notably the frequency and absorbance.\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nSubjects contains information about the participants in each study, including their age, sex, race and ethnicity.\n\nSELECT *\nFROM Subjects\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nAbur_2014\n1\n7\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n3\n8\n19\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 5 not included do to acoustic leak\n\n\nAbur_2014\n4\n7\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n6\n8\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n7\n5\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\n\n\n\nPI_Info contains information about the authors, year, journal and title of each study in the database. This will be useful for including the primary author and date of publication in the replicated figure legend."
  },
  {
    "objectID": "project4.html#replicating-the-figure-from-voss-2020",
    "href": "project4.html#replicating-the-figure-from-voss-2020",
    "title": "SQL",
    "section": "",
    "text": "SELECT Identifier, Frequency, LOG10(Frequency) AS log_frequency,  AVG(Absorbance) AS mean_absorbance \nFROM Measurements\nWHERE Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\") AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Frequency;  \n\n\ngraph |&gt;\nggplot(aes (x=Frequency, y = mean_absorbance, \n   color =  Identifier,\n   group = Identifier)) +\n  geom_line() +\n  scale_x_log10() +\n  labs(title = \"Mean absorbance from each publication in WAI database\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Study\") +\n  xlim(200, 8000) +\n  ylim(0, 1) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis plot is not an identical replication of the figure in Voss’ 2020 study, but it is closely similar. It displays frequency versus mean absorbance measurements for the 12 studies included in the WAI database (as of July 2019). The number of unique ears as well as the equipment used in each study is missing from the legend above. This information is included in the figure I am trying to replicate, so it needs to be displayed here.\n\nSELECT p.Identifier, p.Year, p.AuthorsShortList, Frequency,\nLOG10(Frequency) AS log_frequency, AVG(Absorbance) AS mean_absorbance,\nCOUNT(DISTINCT SubjectNumber, Ear) AS ear_u,\nCONCAT(AuthorsShortList, \" (\" , year, \") \", \"N=\", COUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS legend\nFROM PI_Info AS p\nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier\nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\") AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument, Frequency;\n\n\nplot2 |&gt;\nggplot(aes(x = Frequency, y = mean_absorbance,\n  color = legend,\n  group = legend)) +\n  geom_line() +\n  scale_x_log10() +\n  labs(title = \"Mean absorbance from each publication in WAI database\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Study, No of individual ears, Equipment\") +\n  xlim(200, 8000) +\n  ylim(0, 1) +\n  theme_minimal() +\n  theme(legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8))\n\n\n\n\n\n\n\n\nThis plot is identical to the previous one, except that the legend now includes the number of unique ears and the equipment used in the study."
  },
  {
    "objectID": "project4.html#race-differences-in-mean-absorbance",
    "href": "project4.html#race-differences-in-mean-absorbance",
    "title": "SQL",
    "section": "",
    "text": "SELECT s.Race, m.Frequency, AVG(m.Absorbance) AS mean_absorbance \nFROM Subjects AS s \nRIGHT JOIN Measurements AS m ON s.SubjectNumber = m.SubjectNumber\nWHERE m.Identifier = \"Voss_2010\" AND m.Frequency &gt; 200 AND m.Frequency &lt; 8000\nGROUP BY s.Race, m.Frequency;\n\n\nhead(groups_graph)\n\n              Race Frequency mean_absorbance\n1 African American  210.9375      0.06189557\n2 African American  234.3750      0.03557816\n3 African American  257.8125      0.06311273\n4 African American  281.2500      0.05740714\n5 African American  304.6875      0.06617098\n6 African American  328.1250      0.07566153\n\n\n\ngroups_graph |&gt;\n  ggplot(aes(x = Frequency, y = mean_absorbance)) +\n           geom_line(aes(color = Race)) +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Differences in Mean Absorbance by Race\",\n       subtitle = \"Voss et al., 2010\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Race\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n    theme(legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8),\n        legend.position = c(0.05, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.6, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nI chose to look at differences in mean absorbance for each frequency band by race in one specific study conducted by Voss et al. in 2010. I chose this study because of its large number of participants (1984), which should help to make trends in the data more clear. Each line represents a different race included in the study. It can be seen from the graph that mean absorbance follows very similar trends for White, Caucasian and Asian participants. Meanwhile, the mean absorbances for African American, Black and Mixed participants are noticeably different from those of the other races. Notably, African American participants tend to have a higher mean absorbance for the same frequencies than the other races included in the study, especially for frequencies between 1000 Hz and 5000 Hz. Mixed participants tend to have lower mean absorbances than the other races in the study for frequencies between 2000 Hz and 4000 Hz."
  },
  {
    "objectID": "project4.html#conclusion",
    "href": "project4.html#conclusion",
    "title": "SQL",
    "section": "",
    "text": "In this project I used SQL queries to obtain the necessary information from the relevant tables in the WAI database (Subjects, Measurements and PI_Info). These SQL queries enabled me to save the data as R objects, from which I could produce graphs using ggplot. I was able to replicate Figure 1 from Voss’ 2019 study (Voss 2019), and I also produced a graph that showed clear race differences in mean absorbance for Voss et al.’s 2010 study."
  },
  {
    "objectID": "miniproject3.html",
    "href": "miniproject3.html",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "",
    "text": "View My Final Presentation on this project"
  },
  {
    "objectID": "miniproject3.html#project-overview",
    "href": "miniproject3.html#project-overview",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Project Overview",
    "text": "Project Overview\nThe goal of this analysis is to use a visual lineup protocol to determine whether or not there is a home advantage in professional women’s football. The null hypothesis (single-tailed) is that a team’s home status for a game has no influence on the outcome of that game. The alternative hypothesis is that playing at home is advantageous, which translates to a greater proportion of wins for home teams relative to away teams. To test this hypothesis, I will create a plot showing the relationship between home status and match outcome for the observed data. Then, I will run permutations on the data to generate 20 null plots and I will randomly place the observed data plot among these plots, to see if visual discernment between the observed and permuted data is possible."
  },
  {
    "objectID": "miniproject3.html#eda",
    "href": "miniproject3.html#eda",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "EDA",
    "text": "EDA\nWe can start by selecting only the relevant variables and inspecting what the data look like:\n\nlibrary(tidyverse)\newf_appearances&lt;- ewf_appearances |&gt;\n  select(match_name, date, home_team, away_team, win, loss, draw)\nhead(ewf_appearances)\n\n# A tibble: 6 × 7\n  match_name                    date       home_team away_team   win  loss  draw\n  &lt;chr&gt;                         &lt;date&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Chelsea Ladies vs Arsenal La… 2011-04-13         1         0     0     1     0\n2 Chelsea Ladies vs Arsenal La… 2011-04-13         0         1     1     0     0\n3 Lincoln Ladies vs Doncaster … 2011-04-13         1         0     0     1     0\n4 Lincoln Ladies vs Doncaster … 2011-04-13         0         1     1     0     0\n5 Birmingham City Ladies vs Br… 2011-04-14         1         0     1     0     0\n6 Birmingham City Ladies vs Br… 2011-04-14         0         1     0     1     0\n\newf_appearances&lt;- na.omit(ewf_appearances)\n\nFirst, we can make a contigency table to turn the binary variables in the dataset into numbers that are easier to interpret visually:\n\newf_appearances$result &lt;- ifelse(ewf_appearances$win == 1, \"Win\",\n                        ifelse(ewf_appearances$loss == 1, \"Loss\", \"Draw\"))\n\newf_table &lt;- table(ewf_appearances$home_team, ewf_appearances$result)\nprint(ewf_table)\n\n   \n    Draw Loss  Win\n  0  409 1002  887\n  1  409  887 1002\n\n\n\ndf &lt;- as.data.frame(ewf_table)\ncolnames(df) &lt;- c(\"home\", \"result\", \"frequency\")\n\nggplot(df, aes(x = home, y = result, fill = frequency)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"maroon\", limits = c(0, 1100)) +\n  labs(title = \"Home Status vs. Match Outcome in English Women's Football\",\n       x = \"Home Status\",\n       y = \"Outcome\",\n       fill = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis preliminary figure suggests that there might be a home advantage."
  },
  {
    "objectID": "miniproject3.html#observed-data",
    "href": "miniproject3.html#observed-data",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Observed Data",
    "text": "Observed Data\nWe can alternatively display the data using a stacked bar plot:\n\newf&lt;- ewf_appearances |&gt;\n  select(home_team, result)\n\newf |&gt;\n  ggplot(aes(x = as.factor(home_team), fill = result)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Home Status (1 = Home, 0 = Away)\", y = \"Proportion\", fill = \"Outcome\") +\n  theme_minimal() +\n  ggtitle(\"Home Status vs. Match Result in English Women's Football\")\n\n\n\n\n\n\n\n\nAgain, the data is suggestive of a home advantage since there is a slightly higher proportion of wins when Home Status = 1, i.e. when a team plays at home."
  },
  {
    "objectID": "miniproject3.html#permutation",
    "href": "miniproject3.html#permutation",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Permutation",
    "text": "Permutation\nNext, a permutation function needs to be created to produce a null sampling distribution:\n\nset.seed(3)\npermutation &lt;- function(dataset, perm_n) {\n  dataset |&gt;\n    group_by(match_name) |&gt;\n    mutate(perm_home_team = sample(home_team, replace=FALSE)) |&gt;\n    ungroup() |&gt;\n    mutate(perm_n = perm_n)\n}\n\nshuffles &lt;- map_dfr(1:20, ~ permutation(ewf_appearances, .x))\n\n\n#the data is grouped by match name, and basically permutes the home team for each match, so that it either stays the same or is swapped, the result stays the same\n\n\nshuffles |&gt;\n  ggplot(aes(x = as.factor(perm_home_team), fill = result)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Home Status (1 = Home, 0 = Away)\",\n       y = \"Proportion\",\n       fill = \"Result\") +\n  ggtitle(\"Home Status vs. Match Result in English Women's Football\") +\n  theme_minimal() +\n  facet_wrap(~ perm_n, ncol = 4)\n\n\n\n\n\n\n\n\nThe 20 plots display stacked bar charts for each permutation. Upon visual inspection, the plots look very similar to each other, and there does not appear to be a home advantage, as expected under the null hypothesis."
  },
  {
    "objectID": "miniproject3.html#lineup-protocol",
    "href": "miniproject3.html#lineup-protocol",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Lineup Protocol",
    "text": "Lineup Protocol\nTo execute a lineup protocol, the plot for the observed data should be placed among the permuted data plots. This can be done by assigning a unique permutation number for the observed data, binding the observed and permuted dataframe by row, and shuffling the order of the plots in the facet wrap:\n\nset.seed(3)\n\nobserved &lt;- ewf_appearances |&gt;\n  mutate(perm_home_team = home_team, perm_n = 0) |&gt;\n  select(perm_home_team, result, perm_n)\n\nshuffles &lt;- map_dfr(1:20, ~ permutation(ewf_appearances, .x))|&gt;\n  select(perm_home_team, result, perm_n)\n\nperm_combined &lt;- rbind(observed, shuffles)\n  \nplots&lt;- perm_combined |&gt;\n  mutate(perm_n = factor(perm_n, levels = c(0, 1:20))) |&gt;\n  ggplot(aes(x = as.factor(perm_home_team), fill = result)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Home Team (1 = Home, 0 = Away)\", \n       y = \"Proportion\", \n       fill = \"Result\") +\n  ggtitle(\"Home Status vs. Match Result in English Women's Football\") +\n  theme_minimal() +\n  facet_wrap(~ perm_n, ncol = 4) +\n  theme(\n    plot.title = element_text(size = 100, face = \"bold\"),         \n    axis.title.x = element_text(size = 90),                      \n    axis.title.y = element_text(size = 90),                     \n    axis.text.x = element_text(size = 80),                       \n    axis.text.y = element_text(size = 75),                       \n    legend.title = element_text(size = 90),                      \n    legend.text = element_text(size = 88),                       \n    strip.text = element_text(size = 70)                         \n  )\nprint(plots)\n\n\n\n\n\n\n\n\nUpon visual inspection of these plots, there is no single plot that stands out from the others as showing a much higher win proportion at home. There is some variability between the plots, with some showing a higher proportion of wins at home and others showing the opposite relationship. When we look closer, we can see that plot 0 has a higher proportion of wins at home than all of the other plots except number 2. Plot 0 corresponds to the observed data, while plot 2 represents permuted data. Since there is no obvious difference between these two plots, this implies that there is no significant home advantage in English professional women’s football."
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "",
    "text": "The data used for this analysis are the “Dear Abby” stories from 1985 to 2017, underlying The Pudding’s “30 years of American anxieties” article from November 2018 (https://pudding.cool/2018/11/dearabby/).\nData was accessed via The Pudding’s GitHub, at https://github.com/the-pudding/data/tree/master/dearabby.\nThe dataset consists of 20034 observations, each corresponding to a question sent in to ‘Dear Abby’, a popular advice column.\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")"
  },
  {
    "objectID": "miniproject2.html#data-source",
    "href": "miniproject2.html#data-source",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "",
    "text": "The data used for this analysis are the “Dear Abby” stories from 1985 to 2017, underlying The Pudding’s “30 years of American anxieties” article from November 2018 (https://pudding.cool/2018/11/dearabby/).\nData was accessed via The Pudding’s GitHub, at https://github.com/the-pudding/data/tree/master/dearabby.\nThe dataset consists of 20034 observations, each corresponding to a question sent in to ‘Dear Abby’, a popular advice column.\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")"
  },
  {
    "objectID": "miniproject2.html#eda",
    "href": "miniproject2.html#eda",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "EDA",
    "text": "EDA\nIt seems that the questions in the dataset are already all in lowercase, but I will apply the str_to_lower() function to ensure my analysis is accurate.\n\ndata&lt;-data|&gt;\n  mutate(question_only=str_to_lower(question_only))\n\n\nFamily Words\nIt is reasonable to assume that a number of these questions might relate to family matters, which are often pressing but too taboo to discuss with the family members involved.\nBelow, I use regular expressions to identify the most commonly occurring family members in the questions from 1985 to 2017.\nI am considering ‘wife’, ‘husband’, ‘son’ and ‘daughter’ (and their plural and possessive forms) as these are the most immediate family members.\nI am also including ‘marriage’ and ‘divorce’ since these are likely sources of distress or conflict that could push somebody to submit a question to the column.\n\nwife&lt;-sum(grepl(\"\\\\bwives\\\\b|\\\\bwife'?s?'?\\\\b\", data$question_only))\nhusband&lt;-sum(grepl(\"\\\\bhusband'?s?'?\\\\b\", data$question_only))\nson&lt;-sum(grepl(\"\\\\bson'?s?'?\\\\b\", data$question_only))\ndaughter&lt;-sum(grepl(\"\\\\bdaughter'?s?'?\\\\b\", data$question_only))\nmarriage&lt;-sum(grepl(\"\\\\bmarriage'?s?'?\\\\b|\\\\bmarried\\\\b\", data$question_only))\ndivorce&lt;-sum(grepl(\"\\\\bdivorces?d?\", data$question_only))\nfamily_data&lt;-data.frame(Wife=wife, Husband=husband, Son=son, Daughter=daughter, Marriage=marriage, Divorce=divorce)\n\nThe table below shows the number of questions asked to Abby from 1985 to 2017 that included each term (including its plural or possessive forms).\n\nprint(family_data)\n\n  Wife Husband  Son Daughter Marriage Divorce\n1 2775    5124 2320     2750     4887    1534\n\n\nIt is very interesting to note that ‘wife’, ‘son’ and ‘daughter’ all appeared a similar number of times, while ‘husband’ was by far the most frequent, appearing almost twice as many times as the other terms (5124 times - this is approximately one quarter of the 20034 questions!) ‘Marriage’ was also very common, starring in 4887 questions.\n\n\nNumber of Questions Through the Years\nFor some more exploratory data analysis, I wanted to observe how the number of questions per year changed over time:\n\ny_questions&lt;-data|&gt; group_by(year)|&gt; summarize(n_questions=n())\n\n\nggplot(y_questions, aes(x=year, y=n_questions)) +\n  geom_point(color='pink') +\n  theme_dark() +\n  labs(x=\"Year\",\n       y=\"Number of Questions\",\n       title=\"'Dear Abby' Questions Over The Years\",\n       subtitle=\"From 1985 to 2017\")\n\n\n\n\n\n\n\n\nIt seems like 1985 had an abnormally high number of questions. After this, the number of questions decreased steadily until 1999, when the number of questions per year began to increase again. It did so until 2016, and experienced a steep drop in 2017.\n\n\nQuestion Length\nThen, I was curious about the length of the questions in the dataset:\n\ndata|&gt;\n  select(year, question_only)|&gt;\n  mutate(size=str_length(question_only))|&gt;\n  arrange(desc(size))|&gt;\n  slice_head(n=10)\n\n# A tibble: 10 × 3\n    year question_only                                                      size\n   &lt;dbl&gt; &lt;chr&gt;                                                             &lt;int&gt;\n 1  1988 \"media-wise, this is going to be a tough campaign for barbara bu… 13889\n 2  1995 \"on one page of prison life magazine, an inmate describes his fa… 13411\n 3  1990 \"this story has been entered on the data base in 2 parts. this i… 13225\n 4  1988 \"kitty carlisle hart has spent her life in the\\nspotlight. panel… 11690\n 5  1992 \"this story has been entered on the data base in 3 parts. this i… 11550\n 6  1995 \"arthur donald delacy would have--by all rights, should have--tu… 11087\n 7  1989 \"with his arms stuffed in his pockets to the elbow, the cartooni… 10716\n 8  1995 \"while charles schulz sits at his table, pen in hand, drawing th… 10275\n 9  1994 \"i am so fed up with store employees treating me and my friend l…  8867\n10  1986 \"denise salvaggio's dream is to do for bugs what walt disney did…  8514\n\n\nThe longest question occurred in 1998, with a size of 13889 characters - and it was about Barbara Bush!\nWhat about the shortest question?\n\nq_length&lt;-data|&gt;\n  select(year, question_only)|&gt;\n  mutate(size=str_length(question_only))|&gt;\n  arrange(size)|&gt;\n  slice_head(n=10)\nprint(q_length)\n\n# A tibble: 10 × 3\n    year question_only                                            size\n   &lt;dbl&gt; &lt;chr&gt;                                                   &lt;int&gt;\n 1  1996 \"q: were you alone or by yourself?\"                        33\n 2  1996 \"q: so you were gone until you returned?\"                  39\n 3  1990 \"how would you define old age? getting there\"              43\n 4  1996 \"q: how long have you been a french canadian?\"             44\n 5  1986 \"what would you give a man who has everything? freda\"      51\n 6  1990 \"how would you define success? philosophy major, ucla\"     52\n 7  1996 \"q: the youngest son, the 20-year-old, how old is he?\"     52\n 8  2008 \"it's apathy ... but, who cares? -- malcolm in miami\\n\"    52\n 9  1996 \"q: do you have any children or anything of that kind?\"    53\n10  2010 \"how do you mend a broken heart? -- tears on my pillow\"    53\n\n\nIt occurred in 1996, with the rather ominous: “q: were you alone or by yourself?”. In 2010, someone asked a very hard question, but kept it incredibly succint at 53 characters: “how do you mend a broken heart?– tears on my pillow”.\n\n\n47 - Chirp Chirp!\nAt Pomona College, the number 47 holds special meaning: it is said to be a number that frequently occurs in natural settings. With this in mind, I wanted to determine how often the number 47 appears in this dataset.\n\ndata&lt;-data|&gt;\n  mutate(chirp=str_detect(question_only, \"47\"))\nsum(data$chirp)\n\n[1] 227\n\n\nThe number 47 appears 227 times in the dataset, out of 20034 letters written to Abby."
  },
  {
    "objectID": "miniproject2.html#distinguishing-common-themes-using-word-count",
    "href": "miniproject2.html#distinguishing-common-themes-using-word-count",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "Distinguishing Common Themes Using Word Count",
    "text": "Distinguishing Common Themes Using Word Count\nI would like to determine which words are the most commonly occurring across all the questions to see if any common themes can be extracted.\nFirst, stop words need to be removed - these are common words in the English language that don’t have much meaning. The stop_words dataframe from the tidytext library contains 3 lexicons of stop words.\nWe should also remove the words ‘dear’ and ‘abby’ (and its variants) since those will be at the start of almost every letter.\n\ndear_abby &lt;- c(\"dear\", \"abby\", \"abby's\", \"abbys\")\n\n\nWord Cloud - What Concerned Americans Most Between 1985 and 2017?\nBelow is a word cloud displayed the 150 most common words from 32 years ‘Dear Abby’ letters (excluding stop words):\n\nwc &lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^\\\\D+$\"))|&gt;\n  filter(!word %in% dear_abby)|&gt;\n  group_by(word)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(desc(freq))\n\n\nlibrary(wordcloud2)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nwordcloud(words = wc$word, freq = wc$freq, min.freq = 600, max.words=150, random.order=FALSE, rot.per=0, colors=brewer.pal(6, \"Set2\"), scale = c(5, 1))\n\n\n\n\n\n\n\n\nWe can see from both this word cloud and the earlier analysis done on family words that Americans are very concerned by family matters and family members: ‘husband’, ‘children’, ‘kids’, ‘mom’, ‘mother’, ‘married’, ‘family’, ‘wife’, ‘son’, ‘father’, ‘parents’, ‘sister’, to name a few.\nFrom the word cloud, we can also distinguish the most common emotions: ‘hurt’, ‘afraid’, ‘enjoy’, ‘concerned’, ‘upset’.\n\n\nMost Common Word by Year\n\nwords_by_year&lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^\\\\D+$\"))|&gt;\n  filter(!word %in% dear_abby)|&gt;\n  group_by(word, year)|&gt;\n  summarize(freq=n())|&gt;\n  ungroup()|&gt;\n  group_by(year)|&gt;\n  slice_max(n = 1, order_by=freq)|&gt;\n  arrange(year)\n\n\nggplot(words_by_year, aes(x=year, y=freq)) +\n  geom_point(aes(shape=word, color=word), size=2) +\n  labs(title=\"Most Common Word in 'Dear Abby' Letters\",\n       subtitle= \"By Year, 1985 to 2017\",\n       x=\"Year\",\n       y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHusband Letters Over Time\nSince ‘husband’ was the most common word between 1985 and 2017, it would be interesting to see how its frequency changed over time:\n\nhusband_year&lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^husband'?s?$\"))|&gt;\n  group_by(year)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(year)\n\n\nggplot(husband_year, aes(x=year, y=freq)) +\n  geom_point(color=\"darkgreen\", shape=18, size=2.5) +\n  theme_minimal() +\n  labs(title=\"Frequency of 'Husband' in 'Dear Abby' Letters\",\n       subtitle= \"1985 to 2017\",\n       x=\"Year\",\n       y=\"Number of Occurrences\")\n\n\n\n\n\n\n\n\n\n\nElection Letters Over Time\nLastly, I was interested in determining whether the word ‘election’ (and its plural form) was a common concern. I also wanted to see if it became more frequent during US Presidential Election years.\n\nelections_year&lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^elections?$\"))|&gt;\n  group_by(year)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(year)\n\n\nggplot(elections_year, aes(x=year, y=freq)) +\n  geom_bar(stat=\"identity\", aes(fill = ifelse(year %in% c(1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016), \"Yes\", \"No\"))) +\n  scale_fill_manual(values = c(\"Yes\" = \"navyblue\", \"No\" = \"gray\")) +\n  theme_minimal() +\n  labs(title=\"'Dear Abby' Election Letters\",\n       subtitle=\"Years With and Without US Presidential Elections\",\n       y=\"Year\",\n       x=\"Frequency\",\n       fill=\"Election Year?\")\n\n\n\n\n\n\n\n\nThe word ‘election’ and its plural form ‘elections’ does not seem to be a popular concern, never appearing more than 4 times in one year. However, the plot above shows that these words were more common during US Presidential Election years than during years without a Presidential Election, with the exception of 1994."
  },
  {
    "objectID": "olympics.html",
    "href": "olympics.html",
    "title": "Olympics",
    "section": "",
    "text": "Data for this analysis was accessed from the TidyTuesday GitHub repository at https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-07-27 . Data was scraped from www.sports-reference.com in May 2018 and contains historical Olympics data from Athens 1896 to Rio 2016.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-27/olympics.csv')"
  },
  {
    "objectID": "atp.html",
    "href": "atp.html",
    "title": "How to Win in Tennis",
    "section": "",
    "text": "With an off-season only a month long and a loaded tournament schedule year-round, professional tennis players on the Association of Tennis Professionals (ATP) Tour have limited practice time to improve their game. Understanding how particular match statistics influence a player’s winning percentage can help ‘to determine the match statistics to which coaches should attend in an effort to mould elite professional players’ during this scarce practice time (Reid, McMurtrie, and Crespo 2010). A greater winning percentage leads to gains in ATP world ranking points, which results in direct entry to more prestigious tournaments and consequently increases financial gains through prize money. This is of great importance because of the high annual cost of travel, coaching and equipment, which ‘can total between $121,000 and $197,000 USD’ for male professionals (Reid et al. 2014). Only an elite few earn lucrative sponsorship deals, meaning that the majority of tennis professionals must rely on prize money to finance both their life on tour and their life after retirement from professional tennis.\nWith an off-season only a month long and a loaded tournament schedule year-round, professional tennis players on the Association of Tennis Professionals (ATP) Tour have limited practice time to improve their game. Understanding how particular match statistics influence a player’s winning percentage can help ‘to determine the match statistics to which coaches should attend in an effort to mould elite professional players’ during this scarce practice time (Reid, McMurtrie, and Crespo 2010). A greater winning percentage leads to gains in ATP world ranking points, which results in direct entry to more prestigious tournaments and consequently increases financial gains through prize money. This is of great importance because of the high annual cost of travel, coaching and equipment, which ‘can total between $121,000 and $197,000 USD’ for male professionals (Reid et al. 2014). Only an elite few earn lucrative sponsorship deals, meaning that the majority of tennis professionals must rely on prize money to finance both their life on tour and their life after retirement from professional tennis.\nPrevious analysis of ATP Tour data from the 2007 season carried out by Reid et al. (2010) observed the relationship between 14 tennis match statistics and a player’s world ranking (Reid, McMurtrie, and Crespo 2010). They found that second serve return points won and second serve points won were the most significant predictors of a player’s world ranking. The second serve statistic provides insight on a player’s ability to win points that generally wouldn’t be won: servers take less risk when hitting second serves, as an error here automatically loses them the point. Thus, these are often weaker and less well-placed than first serves, giving the returner a better chance of returning the serve and of winning the point, hence the value of looking at the second serve return statistic as well. In addition to these serve and return statistics, this study will consider players’ winners to unforced error ratio, which is a good indicator of both tactical aggressiveness and consistency, and the percentage of points played at the net, which is another measure of aggressiveness. These statistics were not considered in Reid et al.’s (2010) study (Reid, McMurtrie, and Crespo 2010). This analysis will also differ from the aforementioned study in that the response variable observed will be winning percentage rather than world ranking. Additionally, player age and height will be included in our selection of explanatory variables.\nPrevious literature suggests the presence of a height advantage in men’s professional tennis: in Ovaska et al.’s (2014) analysis of ATP match data from 2000 to 2009, it was found that ‘a higher ranked taller player was 3.1% more likely to win, other things constant,’ than a shorter player (Ovaska, Sumell, and Sumell 2014). However, this ‘advantage diminishes with increasing height’ as at very tall heights players tend to lose mobility (Ovaska, Sumell, and Sumell 2014). The same study found that ‘higher ranked older players have a disadvantage over their younger opponents’ and that ‘the larger the disparity in age, the larger the probability that the older player actually loses’ (Ovaska, Sumell, and Sumell 2014). Of course, a coach and player cannot work on changing height or age, but this previous literature suggests that these factors should be taken into account when attempting to predict a male professional tennis player’s winning percentage.\nPrevious literature suggests the presence of a height advantage in men’s professional tennis: in Ovaska et al.’s (2014) analysis of ATP match data from 2000 to 2009, it was found that ‘a higher ranked taller player was 3.1% more likely to win, other things constant,’ than a shorter player (Ovaska, Sumell, and Sumell 2014). However, this ‘advantage diminishes with increasing height’ as at very tall heights players tend to lose mobility (Ovaska, Sumell, and Sumell 2014). The same study found that ‘higher ranked older players have a disadvantage over their younger opponents’ and that ‘the larger the disparity in age, the larger the probability that the older player actually loses’ (Ovaska, Sumell, and Sumell 2014). Of course, a coach and player cannot work on changing height or age, but this previous literature suggests that these factors should be taken into account when attempting to predict a male professional tennis player’s winning percentage.\nFor this study, we will analyze singles match data from the 2022 ATP Tour to attempt to build a model predicting a player’s match winning percentage in a season based on age, height and 4 match statistics. We aim to show the effect of changes in each match statistic on a touring professional’s winning percentage at a certain height and age, providing guidance for coaches and players on which tactical aspects to work on. The data and variables we will consider will be covered in the methods section, model fitting and inference will be found in the data analysis section, and the conclusion will summarize findings and comment on possible model improvements."
  },
  {
    "objectID": "atp.html#introduction",
    "href": "atp.html#introduction",
    "title": "How to Win in Tennis",
    "section": "",
    "text": "With an off-season only a month long and a loaded tournament schedule year-round, professional tennis players on the Association of Tennis Professionals (ATP) Tour have limited practice time to improve their game. Understanding how particular match statistics influence a player’s winning percentage can help ‘to determine the match statistics to which coaches should attend in an effort to mould elite professional players’ during this scarce practice time (Reid, McMurtrie, and Crespo 2010). A greater winning percentage leads to gains in ATP world ranking points, which results in direct entry to more prestigious tournaments and consequently increases financial gains through prize money. This is of great importance because of the high annual cost of travel, coaching and equipment, which ‘can total between $121,000 and $197,000 USD’ for male professionals (Reid et al. 2014). Only an elite few earn lucrative sponsorship deals, meaning that the majority of tennis professionals must rely on prize money to finance both their life on tour and their life after retirement from professional tennis.\nWith an off-season only a month long and a loaded tournament schedule year-round, professional tennis players on the Association of Tennis Professionals (ATP) Tour have limited practice time to improve their game. Understanding how particular match statistics influence a player’s winning percentage can help ‘to determine the match statistics to which coaches should attend in an effort to mould elite professional players’ during this scarce practice time (Reid, McMurtrie, and Crespo 2010). A greater winning percentage leads to gains in ATP world ranking points, which results in direct entry to more prestigious tournaments and consequently increases financial gains through prize money. This is of great importance because of the high annual cost of travel, coaching and equipment, which ‘can total between $121,000 and $197,000 USD’ for male professionals (Reid et al. 2014). Only an elite few earn lucrative sponsorship deals, meaning that the majority of tennis professionals must rely on prize money to finance both their life on tour and their life after retirement from professional tennis.\nPrevious analysis of ATP Tour data from the 2007 season carried out by Reid et al. (2010) observed the relationship between 14 tennis match statistics and a player’s world ranking (Reid, McMurtrie, and Crespo 2010). They found that second serve return points won and second serve points won were the most significant predictors of a player’s world ranking. The second serve statistic provides insight on a player’s ability to win points that generally wouldn’t be won: servers take less risk when hitting second serves, as an error here automatically loses them the point. Thus, these are often weaker and less well-placed than first serves, giving the returner a better chance of returning the serve and of winning the point, hence the value of looking at the second serve return statistic as well. In addition to these serve and return statistics, this study will consider players’ winners to unforced error ratio, which is a good indicator of both tactical aggressiveness and consistency, and the percentage of points played at the net, which is another measure of aggressiveness. These statistics were not considered in Reid et al.’s (2010) study (Reid, McMurtrie, and Crespo 2010). This analysis will also differ from the aforementioned study in that the response variable observed will be winning percentage rather than world ranking. Additionally, player age and height will be included in our selection of explanatory variables.\nPrevious literature suggests the presence of a height advantage in men’s professional tennis: in Ovaska et al.’s (2014) analysis of ATP match data from 2000 to 2009, it was found that ‘a higher ranked taller player was 3.1% more likely to win, other things constant,’ than a shorter player (Ovaska, Sumell, and Sumell 2014). However, this ‘advantage diminishes with increasing height’ as at very tall heights players tend to lose mobility (Ovaska, Sumell, and Sumell 2014). The same study found that ‘higher ranked older players have a disadvantage over their younger opponents’ and that ‘the larger the disparity in age, the larger the probability that the older player actually loses’ (Ovaska, Sumell, and Sumell 2014). Of course, a coach and player cannot work on changing height or age, but this previous literature suggests that these factors should be taken into account when attempting to predict a male professional tennis player’s winning percentage.\nPrevious literature suggests the presence of a height advantage in men’s professional tennis: in Ovaska et al.’s (2014) analysis of ATP match data from 2000 to 2009, it was found that ‘a higher ranked taller player was 3.1% more likely to win, other things constant,’ than a shorter player (Ovaska, Sumell, and Sumell 2014). However, this ‘advantage diminishes with increasing height’ as at very tall heights players tend to lose mobility (Ovaska, Sumell, and Sumell 2014). The same study found that ‘higher ranked older players have a disadvantage over their younger opponents’ and that ‘the larger the disparity in age, the larger the probability that the older player actually loses’ (Ovaska, Sumell, and Sumell 2014). Of course, a coach and player cannot work on changing height or age, but this previous literature suggests that these factors should be taken into account when attempting to predict a male professional tennis player’s winning percentage.\nFor this study, we will analyze singles match data from the 2022 ATP Tour to attempt to build a model predicting a player’s match winning percentage in a season based on age, height and 4 match statistics. We aim to show the effect of changes in each match statistic on a touring professional’s winning percentage at a certain height and age, providing guidance for coaches and players on which tactical aspects to work on. The data and variables we will consider will be covered in the methods section, model fitting and inference will be found in the data analysis section, and the conclusion will summarize findings and comment on possible model improvements."
  },
  {
    "objectID": "atp.html#methods",
    "href": "atp.html#methods",
    "title": "How to Win in Tennis",
    "section": "Methods",
    "text": "Methods\nMatch statistics and biographical player information used in this analysis were obtained from the Ultimate Tennis Statistics website 1. The statistics found on this website are gathered from the open-source data repository by Jeff Sackmann 2, storing in-match statistics recorded from every ATP match since 1991. The response variable we will try to predict is winning percentage, which is the percentage of matches won by a player during the tennis season (January to November). We consider winning percentage rather than number of matches won to account for the different numbers of matches played across a season by different players due to factors such as injury and tournament schedule. The predictor variables that we will consider are player height in cm (‘height’), player age in years as of the end of 2022 (‘age’), the percentage of second serve points won when returning (‘return’), the percentage of second serve points won when serving (‘serve’), the ratio of winners to unforced errors (‘ratio’), and the percentage of points played at the net (‘net’).\nConsidered in this study were the 100 male professional tennis players with the highest number of singles matches played on the ATP Tour in 2022. This was done because a statistic is more likely to be representative of a player’s skill in that tactical area over a large number of matches played in a variety of conditions, which can affect outcomes. As this is a ranking, the data was not randomly sampled. All of the players in this analysis regularly competed on the ATP Tour during the 2022 season and as such the results will only apply to touring professional male tennis players. Additionally, this analysis will only consider singles tennis, which is tactically very different to doubles. Thus, conclusions drawn from singles data cannot be directly applied to doubles. It must also be noted that the observations in this sample are not independent, as each win must necessarily be accompanied by a loss. However, this dependence is negligible over the course of a season. Additionally, as the sample only contains 100 players out of the population of touring professionals, each player’s winning percentage is influenced by certain wins and losses against other players not sampled, which diminishes this dependence. Each player’s winning percentage is a function of the number of matches he competes in, which varies from player to player. Thus, unless the winning percentage were 100 (which does not occur in the sample), knowing the winning percentage for one player does not reveal information about the winning percentage for another player on the tour. As the dependence of the observations is negligible here, we can conclude that the independence assumption is reasonably fulfilled and proceed with fitting a multiple regression."
  },
  {
    "objectID": "atp.html#data-analysis",
    "href": "atp.html#data-analysis",
    "title": "How to Win in Tennis",
    "section": "Data Analysis",
    "text": "Data Analysis\nThe variables included in the final model were height, serve, ratio, and return cubed, with the slope coefficients and associated p-values displayed in Table 1 and a y-intercept of -207.8. All four predictor variables were significant at the 0.001 level. The adjusted R-squared for the model was 0.6037, with a residual standard error of 8.678. The model can be represented by the following equation:\n Table 1. The predictor variables included in the model along with their slope coefficients and corresponding p-values.\nThis model was arrived at via the following procedure. With assistance from the Box-Cox function in R, it was deemed that no transformation of the winning percentage response variable was necessary. Each of the 6 predictor variables were then input into the Box-Cox function to verify if any transformations were required. The suggested transformations were applied and added to the data frame only if the resulting variable remained interpretable. The transformed variables that were added to the data frame were the reciprocal of age, the square root of ratio, the natural logarithm of net, and return cubed. The updated data frame was then subjected to both stepwise AIC and relaxed LASSO. Both methods suggested adding height, serve, ratio, and return cubed to the model. The addition of other variables was suggested, but these fits yielded insignificant p-values and had negligible effects on adjusted R-squared values. For the sake of model simplicity and interpretability, it was decided to include height, serve, ratio, and return3 in the final model, with 𝛽 values and p-values shown in Table 1.\nThe appropriateness of this multiple linear regression model can be evaluated via the residual plots shown in Figure 1. It can be seen in Figure 1a) that the data follows a linear trend as the average residual locally appears to be around 0 for these fitted values, with the exception of one outlier. Therefore, the linear assumption is satisfactorily fulfilled. Figure 1c) indicates that homoskedasticity is also satisfied as the studentized residuals are structured roughly in a band either side of the red line. This line is not perfectly straight, but from the general distribution of the studentized residuals we can reasonably say that the homoskedasticity assumption is not violated. The Q-Q plot in Figure 1b) shows that the normality of residuals assumption is satisfied. The outlier at observation 4, shown in the residuals vs. leverage plot in Figure 1d), has a high leverage but a Cook’s Distance less than 0.5. This low Cook’s Distance implies that this is a benevolent leverage point that is not changing the response surface in a concerning way. Its omission is therefore not necessary: in fact, a benevolent high-leverage point is desirable as points far away from the rest of the data lead to more accurate slope estimates for the variables in the model.\nThe final model tells us that 0.6037 of the variance in a touring professional’s winning percentage can be explained by a combination of height, the cubed percentage of second serve points won when returning, the ratio of winners to unforced errors, and the percentage of second serve points won when serving. How can this model be used for informative purposes? It tells us that, with the other three predictor variables constant, a unit increase in ratio will increase winning percentage by 2.523, while a unit increase in the percentage of second serve points won when serving will increase winning percentage by 1.398. For every cm in height advantage a player has over his opponents, with all other variables constant, his winning percentage is expected to increase by 0.7023. As the return variable was subjected to a cubic transformation, it is perhaps simpler to see its effect on the winning percentage through an example. If a player of height 180cm, serve 47% and ratio 1.25 increased his return value from 50% to 51%, his winning percentage would increase from 31.06% to 33.73%, based on the model’s fitted values. If instead this same player’s return value remained at 50%, with serve and height constant, his ratio of winners to unforced errors would have to increase from 1.25 to 2.31 to reach this improved winning percentage. With height, return and ratio constant, this same player would need to improve his serve value from 47% to 48.91% to improve his winning percentage from 31.06% to 33.73%. Lastly, with serve, return and ratio constant and equal to the initial values specified above, the model tells us that a height increase from 180 cm to 183.8 cm would improve the winning percentage to 33.73%.\nConfidence intervals can be used to estimate the value of the mean winning percentage for a touring professional with specific predictor variable values. In this case, our confidence interval tells us that we are 95% confident that the mean winning percentage for a touring professional 188cm tall with 50.96% of second serve points won when serving, 51.22% of second serve points won when returning and a 1.073 ratio of winners to unforced errors is expected to lie between 50.29% and 55.64%. Prediction intervals were also obtained for each observation in the sample. These estimate the value of the winning percentage for a specific new observation with defined values for each predictor variable. Constructing a prediction interval using the same predictor variable values as above (188cm, 50.96%, 51.22%, 1.073) tells us that we are 95% confident that the winning percentage for the next touring professional with these predictor values will lie between 35.53% and 70.40%. This interval is much larger than the confidence interval as it accounts for the many sources of noise specific to that tennis player that cannot be explained by the variables in our model.\n Figure 1. Residual plots resulting from the fitted multiple linear regression model."
  },
  {
    "objectID": "atp.html#conclusion",
    "href": "atp.html#conclusion",
    "title": "How to Win in Tennis",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, four variables were found to be significant in explaining variability in match winning percentage. These were height in cm, the ratio of winners to unforced errors, the percentage of second serve points won when serving, and the cubed percentage of second serve points won when returning. A multiple linear regression model was constructed using these four explanatory variables. Evaluation of residual plots concluded that the model was suitably appropriate as it did not violate any of the linear regression assumptions. The adjusted R-squared value of 0.6037 for this fit indicates that 60.37% of the variability in match winning percentage can be explained by the variables included in the model. The remaining variability can be attributed to hundreds of additional in-match statistics that could be considered, as well as the factor of luck, especially as tennis is a sport where the outcome of a handful of key points can determine the result of a match.\nThis follows Reid et al.’s (2010) findings that the percentage of second serves won when both serving and returning are significant in predicting world ranking, as a good ranking is a direct consequence of high winning percentage (Reid, McMurtrie, and Crespo 2010). Additionally, these slope coefficients provide support for Ovaska et al.’s (2014) claim that there is a height advantage in men’s tennis but, contrary to these authors’ findings, does not suggest that this advantage subsists past a certain height (Ovaska, Sumell, and Sumell 2014). If a greater number of very tall touring professionals were sampled, this claim could be better evaluated. The fact that there are very few touring professionals above 200 cm could suggest that the height advantage decreases past a certain point, otherwise professional tennis, like basketball for example, would have higher numbers of very tall players. This work also revealed that a player’s ratio of winners to unforced errors is an important predictor, which had not been tested in either of the aforementioned studies. However, neither the percentage of points played at the net nor age proved significant in predicting winning percentage. This is contrary to Ovaska et al.’s (2014) finding that younger professional tennis players have an advantage over their older counterparts (Ovaska, Sumell, and Sumell 2014).\nImportantly for real-world applications, each of the four predictor variables has a positive slope coefficient, meaning that increasing any of these values has the ability to increase winning percentage. While height is not something that players and coaches can change, knowing that height affects winning percentage suggests that shorter players must seek increased gains in tactical areas compared to their taller counterparts. It is therefore up to the coach and player to decide which of these three significant match statistics to focus on, depending on the player’s current skill set, in order to improve match winning percentage and consequently ranking and prize money earnings.\nAnalysis of outliers also provides important information about the limitations of the model. An outlier with a lower winning percentage than expected was detected. This data point corresponds to Novak Djokovic, one of the most successful male players of all time, winning 22 Grand Slam singles titles 3. The fitted value for a player with his explanatory response values was 95.78%, while his actual winning percentage was 87.23%. This very high fitted value results from his exceptional match statistics over a season. Out of a year’s worth of matches, it is expected for even the most elite players to occasionally underperform and lose, without their match statistics over the year being significantly affected. An added problem that comes from attempting to predict percentages is that there is a floor at 0% and a ceiling at 100% which the model does not take into account. For example, some of the prediction interval values were above 100%, which of course is not possible. This model could be improved by analyzing a response variable that, while not a percentage, can still provide information on a player’s success, for example number of wins per month, or total ranking points won in the year."
  },
  {
    "objectID": "atp.html#footnotes",
    "href": "atp.html#footnotes",
    "title": "How to Win in Tennis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAvailable at Ultimate Tennis Statistics, accessed 3/29/23.↩︎\nAvailable at GitHub - Jeff Sackmann Tennis ATP, accessed 3/29/23. winning 22 Grand Slam singles titles.↩︎\nRetrieved from Sporting News, accessed 3/31/23.↩︎"
  },
  {
    "objectID": "shakespeare.html",
    "href": "shakespeare.html",
    "title": "Shakespeare",
    "section": "",
    "text": "Data was accessed from the TidyTuesday GitHub repositroy at https://github.com/rfordatascience/tidytuesday/tree/master/data/2024/2024-09-17 . The dataset is from shakespeare.mit.edu (via github.com/nrennie/shakespeare), which contains all of William Shakespeare’s plays and poems. Some of the code below is taken from Deepali Kank on GitHub, https://github.com/deepdk/TidyTuesday2024/tree/main/2024/week_38 .\n\nlibrary(tidyverse)\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/romeo_juliet.csv')\n\n\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(devtools)\nlibrary(ggwordcloud)\nlibrary(png)\nlibrary(svglite)"
  },
  {
    "objectID": "shakespeare.html#romeo-juliet",
    "href": "shakespeare.html#romeo-juliet",
    "title": "Shakespeare",
    "section": "Romeo & Juliet",
    "text": "Romeo & Juliet\nFor this TidyTuesday project, I am interested in using text mining to observe the most commonly spoken words by Romeo and Juliet in the famous Shakespeare play. The first step is to filter for only those lines spoken by Romeo or Juliet.\n\nromeo_juliet&lt;-romeo_juliet |&gt;\n  filter(character %in% c(\"Romeo\", \"Juliet\"))\n\nFor any text mining analysis, stop words (the most common words in a language) need to be removed. Bearing in mind that this is dialogue from the 16th century, modern stop words need to be converted in their 16th century equivalents.\n\ncustom_stop_words &lt;- c(\"thou\", \"thy\", \"thee\", \"thine\", \"art\", \"hast\", \"doth\", \"dost\", \"ere\", \"o\",\"hath\")\n\nNext, we use a tidy pipeline to separate each line of dialogue into individual words, remove both modern and 16th century stop words, remove the possessive ‘s’, and remove strings that are entirely numbers using regular expression matching.\n\nword_counts &lt;- romeo_juliet |&gt;\n  unnest_tokens(word, dialogue) |&gt;\n  anti_join(stop_words) %&gt;%\n  filter(!str_detect(word, \"^[0-9]+$\")) |&gt;\n  filter(!word %in% custom_stop_words) |&gt;\n  mutate(word=stringr::str_replace(word, \"'s\", \"\")) |&gt;\n  count(character, word, sort = TRUE)\n\nThen, word counts for each word are computed individually for Romeo and Juliet.\n\njuliet &lt;- word_counts |&gt; \n  filter(character == \"Juliet\")\nhead(juliet)\n\n# A tibble: 6 × 3\n  character word       n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Juliet    romeo     41\n2 Juliet    love      35\n3 Juliet    night     30\n4 Juliet    nurse     20\n5 Juliet    sweet     16\n6 Juliet    tybalt    14\n\n\n\nromeo &lt;- word_counts |&gt; \n  filter(character == \"Romeo\")\nhead(romeo)\n\n# A tibble: 6 × 3\n  character word       n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Romeo     love      52\n2 Romeo     death     22\n3 Romeo     juliet    17\n4 Romeo     fair      15\n5 Romeo     night     15\n6 Romeo     mine      14\n\n\nWe can then display these word counts via word clouds, in the shape of William Shakespeare himself!\n\np1_ro &lt;- ggplot(\n  romeo,\n  aes(\n    label = word, size = n,color = n\n  )\n) +\n  geom_text_wordcloud_area(\n    mask = readPNG(\"/Users/charlotteimbert/Desktop/git/cameraperture.github.io/AlphaShakespeare.png\"),\n    rm_outside = TRUE\n  ) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  scale_color_gradient(low = \"#FF69B4\", high = \"#C41E3A\")\n\nprint(p1_ro)\n\n\n\n\n\n\n\n\nThe word cloud above represents Romeo’s most common words, while the one below displays Juliet’s most common words.\n\np1_ju &lt;- ggplot(\n  juliet,\n  aes(\n    label = word, size = n,color = n\n  )\n) +\n  geom_text_wordcloud_area(\n    mask = readPNG(\"/Users/charlotteimbert/Desktop/git/cameraperture.github.io/AlphaShakespeare.png\"),\n    rm_outside = TRUE\n  ) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  scale_color_gradient(low = \"#FF69B4\", high = \"#C41E3A\")\n\nprint(p1_ju)\n\n\n\n\n\n\n\n\nJuliet’s most common word was Romeo, but Romeo’s most common word was not Juliet - instead, it was ‘love’."
  },
  {
    "objectID": "shakespeare.html#the-data",
    "href": "shakespeare.html#the-data",
    "title": "Shakespeare",
    "section": "",
    "text": "Data was accessed from the TidyTuesday GitHub repositroy at https://github.com/rfordatascience/tidytuesday/tree/master/data/2024/2024-09-17 . The dataset is from shakespeare.mit.edu (via github.com/nrennie/shakespeare), which contains all of William Shakespeare’s plays and poems. Some of the code below is taken from Deepali Kank on GitHub, https://github.com/deepdk/TidyTuesday2024/tree/main/2024/week_38 .\n\nlibrary(tidyverse)\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/romeo_juliet.csv')\n\n\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(devtools)\nlibrary(ggwordcloud)\nlibrary(png)\nlibrary(svglite)"
  },
  {
    "objectID": "premierleague.html#the-data",
    "href": "premierleague.html#the-data",
    "title": "Premier League Soccer",
    "section": "",
    "text": "The dataset for this analysis was accessed via https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-04-04 . The data is from the Premier League Match Data 2021-2022 via Evan Gower on Kaggle. The dataset contains information about soccer matches in the 2021-2022 English Premier League season, including the date, referee, home and away team, full-time home and away goals, home and away fouls, in addition other information for each match.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')"
  },
  {
    "objectID": "premierleague.html#fouls-and-relegation-status",
    "href": "premierleague.html#fouls-and-relegation-status",
    "title": "Premier League Soccer",
    "section": "Fouls and Relegation Status",
    "text": "Fouls and Relegation Status\nI am interested in identifying trends between the total number of fouls a team commits and their relegation status. The bottom three teams at the end of each season are relegated to the Championship. Relegation status for this analysis is based on the season-end Premier League standings in 2022, obtained from the Premier League website at https://www.premierleague.com/tables?co=1&se=418&ha=-1.\nFirstly, listing teams in descending order of total fouls at home:\n\nsoccer_summary&lt;-soccer |&gt;\n  group_by(HomeTeam) |&gt;\n  summarize(totalfouls=sum(HF)) |&gt;\n  arrange(desc(totalfouls))\nhead(soccer_summary)\n\n# A tibble: 6 × 2\n  HomeTeam       totalfouls\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Leeds                 236\n2 Watford               215\n3 Chelsea               213\n4 Crystal Palace        210\n5 Newcastle             209\n6 Brighton              208\n\n\nNow, plotting this relationship and highlighting the teams that were relegated at the end of the season, in navy blue:\n\nplot&lt;-ggplot(soccer_summary, aes(x=reorder(HomeTeam, totalfouls), y=totalfouls)) +\n  geom_bar(stat = \"identity\", aes(fill = ifelse(HomeTeam %in% c(\"Burnley\", \"Norwich\", \"Watford\"), \"Relegated\", \"Safe\"))) +\n  scale_fill_manual(values = c(\"Relegated\" = \"navyblue\", \"Safe\" = \"lightblue\")) +\n  theme_minimal() +\n  labs(title=\"Premier League Fouls by Team, 2021-2022\",\n       subtitle=\"Home Games\",\n       y=\"Total Number of Fouls\",\n       x=\"Team\",\n       fill=\"Relegation Status\")\nplot + coord_flip()\n\n\n\n\n\n\n\n\nHorizontal bar plot showing the total number of fouls in home games for each team in the 2021-2022 Premier League season. Teams that were relegated at the end of the season are colored in navy blue.\nIs this relationship different when teams are playing away from home?\n\nsoccer_summary2&lt;- soccer |&gt;\n  group_by(AwayTeam) |&gt;\n  summarize(totalfouls_a=sum(AF)) |&gt;\n  arrange(desc(totalfouls_a))\nhead(soccer_summary2)\n\n# A tibble: 6 × 2\n  AwayTeam       totalfouls_a\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Leeds                   233\n2 Man United              226\n3 Watford                 225\n4 Aston Villa             211\n5 Crystal Palace          205\n6 Everton                 201\n\n\n\nplot2&lt;-ggplot(soccer_summary2, aes(x=reorder(AwayTeam, totalfouls_a), y=totalfouls_a)) +\n  geom_bar(stat = \"identity\", aes(fill = ifelse(AwayTeam %in% c(\"Burnley\", \"Norwich\", \"Watford\"), \"Relegated\", \"Safe\"))) +\n  scale_fill_manual(values = c(\"Relegated\" = \"red\", \"Safe\" = \"lightpink\")) +\n  theme_minimal() +\n  labs(title=\"Premier League Fouls by Team, 2021-2022\",\n       subtitle=\"Away Games\",\n       y=\"Total Number of Fouls\",\n       x=\"Team\",\n       fill=\"Relegation Status\")\nplot2 + coord_flip()\n\n\n\n\n\n\n\n\nHorizontal bar plot showing the total number of fouls in away games for each team in the 2021-2022 Premier League season. Teams that were relegated at the end of the season are colored in red."
  },
  {
    "objectID": "premierleague.html#conclusion",
    "href": "premierleague.html#conclusion",
    "title": "Premier League Soccer",
    "section": "Conclusion",
    "text": "Conclusion\nBased on these plots, a team’s total number of fouls does not seem to influence their relegation status. For example, Norwich had one of the lowest number of fouls away from home but still faced relegation.\nWe can also see that home status does not really influence the total number of fouls committed by a team: Leeds and Manchester City are top and bottom, respectively, for the number of fouls both at home and away. The majority of the teams in the league committed a similar total number of fouls at home and away. Manchester United are an exception as they committed the second highest number of fouls away, compared to the fifth lowest at home."
  },
  {
    "objectID": "olympics.html#the-data",
    "href": "olympics.html#the-data",
    "title": "Olympics",
    "section": "",
    "text": "Data for this analysis was accessed from the TidyTuesday GitHub repository at https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-07-27 . Data was scraped from www.sports-reference.com in May 2018 and contains historical Olympics data from Athens 1896 to Rio 2016.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-27/olympics.csv')"
  },
  {
    "objectID": "olympics.html#visualizations",
    "href": "olympics.html#visualizations",
    "title": "Olympics",
    "section": "Visualizations",
    "text": "Visualizations\nI am interested in observing several trends in Olympic data over time. The first is the trend in female Olympians over time, and the second is the trend in Olympians’ ages over time.\nThe scatter plot below aims to examine both trends in one visualization. We can see that the number of female athletes considerably increased over the years, especially since the 1980s. An interesting trend is that there are far more young (below the age of 25) female than male Olympians. Above this age, there is no clear trend is male or female representation.\n\nggplot(olympics, aes(x=year, y=age)) +\n  geom_point(aes(color=sex)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nScatter plot showing the age of athletes over the years from 1896 to 2016. Male athletes are represented in blue, and female athletes are represented in red.\nThis data can alternatively be presented through a stacked histogram. This visualization emphasizes the disparity in the number of female Olympians compared to male Olympians.\n\nggplot(olympics, aes(x=age)) +\n  geom_histogram(aes(fill=sex)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nStacked histogram showing the number of Olympic athletes for each age (by year), for all Olympians between 1896 and 2016. Male athletes are represented in blue, and female athletes are represented in red.\n\nolympics_nona&lt;- olympics[!is.na(olympics$age), ]\nolympics_meanage&lt;- olympics_nona |&gt;\n  group_by(year) |&gt;\n  summarize(avg_age=(mean(age)))\n\nThis final visualization aims to display the trend in the mean age of Olympians over time. An overarching trend that can be seen is that the mean age of athletes increased from 1896 to 1932; it then decreased steadily until 1980, and has been increasing steadily since then. Two outliers are worth pointing out. The very first Olympics in 1896 has an unusually low mean age of approximately 23.5 years, while the 1932 Olympics had an unusually high mean age of around 32.5 years.\n\nggplot(olympics_meanage, aes(x=year, y=avg_age)) +\n  geom_point(shape=16, size=2, color=\"goldenrod4\") +\n  geom_smooth(color=\"#90EE90\", fill=\"#90EE90\", se=FALSE) +\n  theme_minimal() +\n  labs(title = \"Mean Age of Athletes at the Olympics\",\n       subtitle = \"1896 to 2016\",\n       x=\"Olympic Year\",\n       y=\"Mean Age\")\n\n\n\n\n\n\n\n\nScatter plot showing the mean age of Olympic athletes from 1896 to 2016."
  },
  {
    "objectID": "presentation.html#outline",
    "href": "presentation.html#outline",
    "title": "Final Presentation",
    "section": "Outline",
    "text": "Outline\n\nPermutation test"
  },
  {
    "objectID": "presentation.html#the-data",
    "href": "presentation.html#the-data",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "The Data",
    "text": "The Data\n\nThe data used in this analysis comes from the English Women’s Football Database, created by Rob Clapp\nContains data from every match played in the top division (since 2011) and the second division (since 2014) of professional English women’s football"
  },
  {
    "objectID": "presentation.html#the-data-1",
    "href": "presentation.html#the-data-1",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "The Data",
    "text": "The Data\n\nlibrary(tidyverse)\newf_appearances&lt;- ewf_appearances |&gt;\n  select(match_name, date, home_team, away_team, win, loss, draw)\nhead(ewf_appearances)\n\n# A tibble: 6 × 7\n  match_name                    date       home_team away_team   win  loss  draw\n  &lt;chr&gt;                         &lt;date&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Chelsea Ladies vs Arsenal La… 2011-04-13         1         0     0     1     0\n2 Chelsea Ladies vs Arsenal La… 2011-04-13         0         1     1     0     0\n3 Lincoln Ladies vs Doncaster … 2011-04-13         1         0     0     1     0\n4 Lincoln Ladies vs Doncaster … 2011-04-13         0         1     1     0     0\n5 Birmingham City Ladies vs Br… 2011-04-14         1         0     1     0     0\n6 Birmingham City Ladies vs Br… 2011-04-14         0         1     0     1     0"
  },
  {
    "objectID": "presentation.html#eda",
    "href": "presentation.html#eda",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "presentation.html#eda-1",
    "href": "presentation.html#eda-1",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "presentation.html#question",
    "href": "presentation.html#question",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Question",
    "text": "Question\n \n\nIs there a home advantage in English women’s football?"
  },
  {
    "objectID": "presentation.html#permutation",
    "href": "presentation.html#permutation",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Permutation",
    "text": "Permutation\nNull hypothesis: in professional women’s football, a team’s home status for a game has no influence on the outcome of the game \nTest statistic: win proportion\n\nset.seed(3)\npermutation &lt;- function(dataset, perm_n) {\n  dataset |&gt;\n    group_by(match_name) |&gt;\n    mutate(perm_home_team = sample(home_team, replace=FALSE)) |&gt;\n    ungroup() |&gt;\n    mutate(perm_n = perm_n)\n}\n\nshuffles &lt;- map_dfr(1:20, ~ permutation(ewf_appearances, .x))"
  },
  {
    "objectID": "presentation.html#visual-lineup",
    "href": "presentation.html#visual-lineup",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Visual Lineup",
    "text": "Visual Lineup"
  },
  {
    "objectID": "presentation.html#conclusions",
    "href": "presentation.html#conclusions",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Conclusions",
    "text": "Conclusions\n\nNo single plot stands out from the others as showing a much higher win proportion at home\nVisually, the null sampling distribution is similar to the observed data\nPlot 0 (observed data) has a higher proportion of wins at home than all of the other plots except number 2, which implies that this win proportion is entirely possible under the null\nVisual lineup implies that there is no significant home advantage in English professional women’s football"
  },
  {
    "objectID": "project4.html#replicating-the-figure-from-voss-2019",
    "href": "project4.html#replicating-the-figure-from-voss-2019",
    "title": "SQL",
    "section": "",
    "text": "SELECT p.Identifier, p.Year, p.AuthorsShortList, Frequency,\nLOG10(Frequency) AS log_frequency, AVG(Absorbance) AS mean_absorbance,\nCOUNT(DISTINCT SubjectNumber, Ear) AS ear_u,\nCONCAT(AuthorsShortList, \" (\" , year, \") \", \"N=\", COUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS legend\nFROM PI_Info AS p\nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier\nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\") AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument, Frequency;\n\n\nplot |&gt;\nggplot(aes(x = Frequency, y = mean_absorbance,\n  color = legend,\n  group = legend)) +\n  geom_line() +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Mean absorbance from each publication in WAI database\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Study, No of individual ears, Equipment\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n  theme(legend.text = element_text(size = 4),\n        legend.title = element_text(size = 6),\n        legend.position = c(0.01, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.4, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nThis plot above is a replication of the figure in Voss’ 2019 study (Voss 2019). It displays frequency versus mean absorbance measurements for the 12 studies included in the WAI database (as of July 2019). The authors, number of unique ears and equipment used for each study is indicated in the legend in the top left."
  }
]