[
  {
    "objectID": "premierleague.html",
    "href": "premierleague.html",
    "title": "Premier League Soccer",
    "section": "",
    "text": "The dataset for this analysis was accessed via https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-04-04 . The data is from the Premier League Match Data 2021-2022 via Evan Gower on Kaggle. The dataset contains information about soccer matches in the 2021-2022 English Premier League season, including the date, referee, home and away team, full-time home and away goals, home and away fouls, in addition other information for each match.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')"
  },
  {
    "objectID": "premierleague.html#the-data",
    "href": "premierleague.html#the-data",
    "title": "Premier League Soccer",
    "section": "",
    "text": "The dataset for this analysis was accessed via https://github.com/rfordatascience/tidytuesday/tree/master/data/2023/2023-04-04 . The data is from the Premier League Match Data 2021-2022 via Evan Gower on Kaggle. The dataset contains information about soccer matches in the 2021-2022 English Premier League season, including the date, referee, home and away team, full-time home and away goals, home and away fouls, in addition other information for each match.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nsoccer &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-04-04/soccer21-22.csv')"
  },
  {
    "objectID": "premierleague.html#fouls-and-relegation-status",
    "href": "premierleague.html#fouls-and-relegation-status",
    "title": "Premier League Soccer",
    "section": "Fouls and Relegation Status",
    "text": "Fouls and Relegation Status\nI am interested in identifying trends between the total number of fouls a team commits and their relegation status. The bottom three teams at the end of each season are relegated to the Championship. Relegation status for this analysis is based on the season-end Premier League standings in 2022, obtained from the Premier League website at https://www.premierleague.com/tables?co=1&se=418&ha=-1.\nFirstly, listing teams in descending order of total fouls at home:\n\nsoccer_summary&lt;-soccer |&gt;\n  group_by(HomeTeam) |&gt;\n  summarize(totalfouls=sum(HF)) |&gt;\n  arrange(desc(totalfouls))\nhead(soccer_summary)\n\n# A tibble: 6 × 2\n  HomeTeam       totalfouls\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Leeds                 236\n2 Watford               215\n3 Chelsea               213\n4 Crystal Palace        210\n5 Newcastle             209\n6 Brighton              208\n\n\nNow, plotting this relationship and highlighting the teams that were relegated at the end of the season, in navy blue:\n\nplot&lt;-ggplot(soccer_summary, aes(x=reorder(HomeTeam, totalfouls), y=totalfouls)) +\n  geom_bar(stat = \"identity\", aes(fill = ifelse(HomeTeam %in% c(\"Burnley\", \"Norwich\", \"Watford\"), \"Relegated\", \"Safe\"))) +\n  scale_fill_manual(values = c(\"Relegated\" = \"navyblue\", \"Safe\" = \"lightblue\")) +\n  theme_minimal() +\n  labs(title=\"Premier League Fouls by Team, 2021-2022\",\n       subtitle=\"Home Games\",\n       y=\"Total Number of Fouls\",\n       x=\"Team\",\n       fill=\"Relegation Status\")\nplot + coord_flip()\n\n\n\n\n\n\n\n\nHorizontal bar plot showing the total number of fouls in home games for each team in the 2021-2022 Premier League season. Teams that were relegated at the end of the season are colored in navy blue.\nIs this relationship different when teams are playing away from home?\n\nsoccer_summary2&lt;- soccer |&gt;\n  group_by(AwayTeam) |&gt;\n  summarize(totalfouls_a=sum(AF)) |&gt;\n  arrange(desc(totalfouls_a))\nhead(soccer_summary2)\n\n# A tibble: 6 × 2\n  AwayTeam       totalfouls_a\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Leeds                   233\n2 Man United              226\n3 Watford                 225\n4 Aston Villa             211\n5 Crystal Palace          205\n6 Everton                 201\n\n\n\nplot2&lt;-ggplot(soccer_summary2, aes(x=reorder(AwayTeam, totalfouls_a), y=totalfouls_a)) +\n  geom_bar(stat = \"identity\", aes(fill = ifelse(AwayTeam %in% c(\"Burnley\", \"Norwich\", \"Watford\"), \"Relegated\", \"Safe\"))) +\n  scale_fill_manual(values = c(\"Relegated\" = \"red\", \"Safe\" = \"lightpink\")) +\n  theme_minimal() +\n  labs(title=\"Premier League Fouls by Team, 2021-2022\",\n       subtitle=\"Away Games\",\n       y=\"Total Number of Fouls\",\n       x=\"Team\",\n       fill=\"Relegation Status\")\nplot2 + coord_flip()\n\n\n\n\n\n\n\n\nHorizontal bar plot showing the total number of fouls in away games for each team in the 2021-2022 Premier League season. Teams that were relegated at the end of the season are colored in red."
  },
  {
    "objectID": "premierleague.html#conclusion",
    "href": "premierleague.html#conclusion",
    "title": "Premier League Soccer",
    "section": "Conclusion",
    "text": "Conclusion\nBased on these plots, a team’s total number of fouls does not seem to influence their relegation status. For example, Norwich had one of the lowest number of fouls away from home but still faced relegation.\nWe can also see that home status does not really influence the total number of fouls committed by a team: Leeds and Manchester City are top and bottom, respectively, for the number of fouls both at home and away. The majority of the teams in the league committed a similar total number of fouls at home and away. Manchester United are an exception as they committed the second highest number of fouls away, compared to the fifth lowest at home."
  },
  {
    "objectID": "olympics.html",
    "href": "olympics.html",
    "title": "Olympics",
    "section": "",
    "text": "Data for this analysis was accessed from the TidyTuesday GitHub repository at https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-07-27 . Data was scraped from www.sports-reference.com in May 2018 and contains historical Olympics data from Athens 1896 to Rio 2016.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-27/olympics.csv')"
  },
  {
    "objectID": "olympics.html#the-data",
    "href": "olympics.html#the-data",
    "title": "Olympics",
    "section": "",
    "text": "Data for this analysis was accessed from the TidyTuesday GitHub repository at https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-07-27 . Data was scraped from www.sports-reference.com in May 2018 and contains historical Olympics data from Athens 1896 to Rio 2016.\n\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nolympics &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-07-27/olympics.csv')"
  },
  {
    "objectID": "olympics.html#visualizations",
    "href": "olympics.html#visualizations",
    "title": "Olympics",
    "section": "Visualizations",
    "text": "Visualizations\nI am interested in observing several trends in Olympic data over time. The first is the trend in female Olympians over time, and the second is the trend in Olympians’ ages over time.\nThe scatter plot below aims to examine both trends in one visualization. We can see that the number of female athletes considerably increased over the years, especially since the 1980s. An interesting trend is that there are far more young (below the age of 25) female than male Olympians. Above this age, there is no clear trend is male or female representation.\n\nggplot(olympics, aes(x=year, y=age)) +\n  geom_point(aes(color=sex)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nScatter plot showing the age of athletes over the years from 1896 to 2016. Male athletes are represented in blue, and female athletes are represented in red.\nThis data can alternatively be presented through a stacked histogram. This visualization emphasizes the disparity in the number of female Olympians compared to male Olympians.\n\nggplot(olympics, aes(x=age)) +\n  geom_histogram(aes(fill=sex)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nStacked histogram showing the number of Olympic athletes for each age (by year), for all Olympians between 1896 and 2016. Male athletes are represented in blue, and female athletes are represented in red.\n\nolympics_nona&lt;- olympics[!is.na(olympics$age), ]\nolympics_meanage&lt;- olympics_nona |&gt;\n  group_by(year) |&gt;\n  summarize(avg_age=(mean(age)))\n\nThis final visualization aims to display the trend in the mean age of Olympians over time. An overarching trend that can be seen is that the mean age of athletes increased from 1896 to 1932; it then decreased steadily until 1980, and has been increasing steadily since then. Two outliers are worth pointing out. The very first Olympics in 1896 has an unusually low mean age of approximately 23.5 years, while the 1932 Olympics had an unusually high mean age of around 32.5 years.\n\nggplot(olympics_meanage, aes(x=year, y=avg_age)) +\n  geom_point(shape=16, size=2, color=\"goldenrod4\") +\n  geom_smooth(color=\"#90EE90\", fill=\"#90EE90\", se=FALSE) +\n  theme_minimal() +\n  labs(title = \"Mean Age of Athletes at the Olympics\",\n       subtitle = \"1896 to 2016\",\n       x=\"Olympic Year\",\n       y=\"Mean Age\")\n\n\n\n\n\n\n\n\nScatter plot showing the mean age of Olympic athletes from 1896 to 2016."
  },
  {
    "objectID": "presentation.html#the-data",
    "href": "presentation.html#the-data",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "The Data",
    "text": "The Data\n\nThe data used in this analysis comes from the English Women’s Football Database, created by Rob Clapp\nContains data from every match played in the top division (since 2011) and the second division (since 2014) of professional English women’s football"
  },
  {
    "objectID": "presentation.html#the-data-1",
    "href": "presentation.html#the-data-1",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "The Data",
    "text": "The Data\n\nlibrary(tidyverse)\newf_appearances&lt;- ewf_appearances |&gt;\n  select(match_name, date, home_team, away_team, win, loss, draw)\nhead(ewf_appearances)\n\n# A tibble: 6 × 7\n  match_name                    date       home_team away_team   win  loss  draw\n  &lt;chr&gt;                         &lt;date&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Chelsea Ladies vs Arsenal La… 2011-04-13         1         0     0     1     0\n2 Chelsea Ladies vs Arsenal La… 2011-04-13         0         1     1     0     0\n3 Lincoln Ladies vs Doncaster … 2011-04-13         1         0     0     1     0\n4 Lincoln Ladies vs Doncaster … 2011-04-13         0         1     1     0     0\n5 Birmingham City Ladies vs Br… 2011-04-14         1         0     1     0     0\n6 Birmingham City Ladies vs Br… 2011-04-14         0         1     0     1     0"
  },
  {
    "objectID": "presentation.html#eda",
    "href": "presentation.html#eda",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "presentation.html#eda-1",
    "href": "presentation.html#eda-1",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "EDA",
    "text": "EDA"
  },
  {
    "objectID": "presentation.html#question",
    "href": "presentation.html#question",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Question",
    "text": "Question\n \n\nIs there a home advantage in English women’s football?"
  },
  {
    "objectID": "presentation.html#permutation",
    "href": "presentation.html#permutation",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Permutation",
    "text": "Permutation\nNull hypothesis: in professional women’s football, a team’s home status for a game has no influence on the outcome of the game \nTest statistic: win proportion\n\nset.seed(3)\npermutation &lt;- function(dataset, perm_n) {\n  dataset |&gt;\n    group_by(match_name) |&gt;\n    mutate(perm_home_team = sample(home_team, replace=FALSE)) |&gt;\n    ungroup() |&gt;\n    mutate(perm_n = perm_n)\n}\n\nshuffles &lt;- map_dfr(1:20, ~ permutation(ewf_appearances, .x))"
  },
  {
    "objectID": "presentation.html#visual-lineup",
    "href": "presentation.html#visual-lineup",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Visual Lineup",
    "text": "Visual Lineup"
  },
  {
    "objectID": "presentation.html#conclusions",
    "href": "presentation.html#conclusions",
    "title": "Visual Inference on English Women’s Football Data",
    "section": "Conclusions",
    "text": "Conclusions\n\nNo single plot stands out from the others as showing a much higher win proportion at home\nVisually, the null sampling distribution is similar to the observed data\nPlot 0 (observed data) has a higher proportion of wins at home than all of the other plots except number 2, which implies that this win proportion is entirely possible under the null\nVisual lineup implies that there is no significant home advantage in English professional women’s football"
  },
  {
    "objectID": "miniproject3.html",
    "href": "miniproject3.html",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "",
    "text": "View My Final Presentation on this project"
  },
  {
    "objectID": "miniproject3.html#project-overview",
    "href": "miniproject3.html#project-overview",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Project Overview",
    "text": "Project Overview\nThe goal of this analysis is to use a visual lineup protocol to determine whether or not there is a home advantage in professional women’s football. The null hypothesis (single-tailed) is that a team’s home status for a game has no influence on the outcome of that game. The alternative hypothesis is that playing at home is advantageous, which translates to a greater proportion of wins for home teams relative to away teams. To test this hypothesis, I will create a plot showing the relationship between home status and match outcome for the observed data. Then, I will run permutations on the data to generate 20 null plots and I will randomly place the observed data plot among these plots, to see if visual discernment between the observed and permuted data is possible."
  },
  {
    "objectID": "miniproject3.html#eda",
    "href": "miniproject3.html#eda",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "EDA",
    "text": "EDA\nWe can start by selecting only the relevant variables and inspecting what the data look like:\n\nlibrary(tidyverse)\newf_appearances&lt;- ewf_appearances |&gt;\n  select(match_name, date, home_team, away_team, win, loss, draw)\nhead(ewf_appearances)\n\n# A tibble: 6 × 7\n  match_name                    date       home_team away_team   win  loss  draw\n  &lt;chr&gt;                         &lt;date&gt;         &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Chelsea Ladies vs Arsenal La… 2011-04-13         1         0     0     1     0\n2 Chelsea Ladies vs Arsenal La… 2011-04-13         0         1     1     0     0\n3 Lincoln Ladies vs Doncaster … 2011-04-13         1         0     0     1     0\n4 Lincoln Ladies vs Doncaster … 2011-04-13         0         1     1     0     0\n5 Birmingham City Ladies vs Br… 2011-04-14         1         0     1     0     0\n6 Birmingham City Ladies vs Br… 2011-04-14         0         1     0     1     0\n\newf_appearances&lt;- na.omit(ewf_appearances)\n\nFirst, we can make a contigency table to turn the binary variables in the dataset into numbers that are easier to interpret visually:\n\newf_appearances$result &lt;- ifelse(ewf_appearances$win == 1, \"Win\",\n                        ifelse(ewf_appearances$loss == 1, \"Loss\", \"Draw\"))\n\newf_table &lt;- table(ewf_appearances$home_team, ewf_appearances$result)\nprint(ewf_table)\n\n   \n    Draw Loss  Win\n  0  409 1002  887\n  1  409  887 1002\n\n\n\ndf &lt;- as.data.frame(ewf_table)\ncolnames(df) &lt;- c(\"home\", \"result\", \"frequency\")\n\nggplot(df, aes(x = home, y = result, fill = frequency)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"maroon\", limits = c(0, 1100)) +\n  labs(title = \"Home Status vs. Match Outcome in English Women's Football\",\n       x = \"Home Status\",\n       y = \"Outcome\",\n       fill = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThis preliminary figure suggests that there might be a home advantage."
  },
  {
    "objectID": "miniproject3.html#observed-data",
    "href": "miniproject3.html#observed-data",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Observed Data",
    "text": "Observed Data\nWe can alternatively display the data using a stacked bar plot:\n\newf&lt;- ewf_appearances |&gt;\n  select(home_team, result)\n\newf |&gt;\n  ggplot(aes(x = as.factor(home_team), fill = result)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Home Status (1 = Home, 0 = Away)\", y = \"Proportion\", fill = \"Outcome\") +\n  theme_minimal() +\n  ggtitle(\"Home Status vs. Match Result in English Women's Football\")\n\n\n\n\n\n\n\n\nAgain, the data is suggestive of a home advantage since there is a slightly higher proportion of wins when Home Status = 1, i.e. when a team plays at home."
  },
  {
    "objectID": "miniproject3.html#permutation",
    "href": "miniproject3.html#permutation",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Permutation",
    "text": "Permutation\nNext, a permutation function needs to be created to produce a null sampling distribution:\n\nset.seed(3)\npermutation &lt;- function(dataset, perm_n) {\n  dataset |&gt;\n    group_by(match_name) |&gt;\n    mutate(perm_home_team = sample(home_team, replace=FALSE)) |&gt;\n    ungroup() |&gt;\n    mutate(perm_n = perm_n)\n}\n\nshuffles &lt;- map_dfr(1:20, ~ permutation(ewf_appearances, .x))\n\n\n#the data is grouped by match name, and basically permutes the home team for each match, so that it either stays the same or is swapped, the result stays the same\n\n\nshuffles |&gt;\n  ggplot(aes(x = as.factor(perm_home_team), fill = result)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Home Status (1 = Home, 0 = Away)\",\n       y = \"Proportion\",\n       fill = \"Result\") +\n  ggtitle(\"Home Status vs. Match Result in English Women's Football\") +\n  theme_minimal() +\n  facet_wrap(~ perm_n, ncol = 4)\n\n\n\n\n\n\n\n\nThe 20 plots display stacked bar charts for each permutation. Upon visual inspection, the plots look very similar to each other, and there does not appear to be a home advantage, as expected under the null hypothesis."
  },
  {
    "objectID": "miniproject3.html#lineup-protocol",
    "href": "miniproject3.html#lineup-protocol",
    "title": "Permutation Testing, Iteration and Visual Inference",
    "section": "Lineup Protocol",
    "text": "Lineup Protocol\nTo execute a lineup protocol, the plot for the observed data should be placed among the permuted data plots. This can be done by assigning a unique permutation number for the observed data, binding the observed and permuted dataframe by row, and shuffling the order of the plots in the facet wrap:\n\nset.seed(3)\n\nobserved &lt;- ewf_appearances |&gt;\n  mutate(perm_home_team = home_team, perm_n = 0) |&gt;\n  select(perm_home_team, result, perm_n)\n\nshuffles &lt;- map_dfr(1:20, ~ permutation(ewf_appearances, .x))|&gt;\n  select(perm_home_team, result, perm_n)\n\nperm_combined &lt;- rbind(observed, shuffles)\n  \nplots&lt;- perm_combined |&gt;\n  mutate(perm_n = factor(perm_n, levels = c(0, 1:20))) |&gt;\n  ggplot(aes(x = as.factor(perm_home_team), fill = result)) +\n  geom_bar(position = \"fill\") +\n  labs(x = \"Home Team (1 = Home, 0 = Away)\", \n       y = \"Proportion\", \n       fill = \"Result\") +\n  ggtitle(\"Home Status vs. Match Result in English Women's Football\") +\n  theme_minimal() +\n  facet_wrap(~ perm_n, ncol = 4) +\n  theme(\n    plot.title = element_text(size = 100, face = \"bold\"),         \n    axis.title.x = element_text(size = 90),                      \n    axis.title.y = element_text(size = 90),                     \n    axis.text.x = element_text(size = 80),                       \n    axis.text.y = element_text(size = 75),                       \n    legend.title = element_text(size = 90),                      \n    legend.text = element_text(size = 88),                       \n    strip.text = element_text(size = 70)                         \n  )\nprint(plots)\n\n\n\n\n\n\n\n\nUpon visual inspection of these plots, there is no single plot that stands out from the others as showing a much higher win proportion at home. There is some variability between the plots, with some showing a higher proportion of wins at home and others showing the opposite relationship. When we look closer, we can see that plot 0 has a higher proportion of wins at home than all of the other plots except number 2. Plot 0 corresponds to the observed data, while plot 2 represents permuted data. Since there is no obvious difference between these two plots, this implies that there is no significant home advantage in English professional women’s football."
  },
  {
    "objectID": "miniproject2.html",
    "href": "miniproject2.html",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "",
    "text": "The data used for this analysis are the “Dear Abby” stories from 1985 to 2017, underlying The Pudding’s “30 years of American anxieties” article from November 2018 (https://pudding.cool/2018/11/dearabby/).\nData was accessed via The Pudding’s GitHub, at https://github.com/the-pudding/data/tree/master/dearabby.\nThe dataset consists of 20034 observations, each corresponding to a question sent in to ‘Dear Abby’, a popular advice column.\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")"
  },
  {
    "objectID": "miniproject2.html#data-source",
    "href": "miniproject2.html#data-source",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "",
    "text": "The data used for this analysis are the “Dear Abby” stories from 1985 to 2017, underlying The Pudding’s “30 years of American anxieties” article from November 2018 (https://pudding.cool/2018/11/dearabby/).\nData was accessed via The Pudding’s GitHub, at https://github.com/the-pudding/data/tree/master/dearabby.\nThe dataset consists of 20034 observations, each corresponding to a question sent in to ‘Dear Abby’, a popular advice column.\n\nlibrary(tidyverse)\ndata&lt;-read_csv(\"https://raw.githubusercontent.com/the-pudding/data/master/dearabby/raw_da_qs.csv\")"
  },
  {
    "objectID": "miniproject2.html#eda",
    "href": "miniproject2.html#eda",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "EDA",
    "text": "EDA\nIt seems that the questions in the dataset are already all in lowercase, but I will apply the str_to_lower() function to ensure my analysis is accurate.\n\ndata&lt;-data|&gt;\n  mutate(question_only=str_to_lower(question_only))\n\n\nFamily Words\nIt is reasonable to assume that a number of these questions might relate to family matters, which are often pressing but too taboo to discuss with the family members involved.\nBelow, I use regular expressions to identify the most commonly occurring family members in the questions from 1985 to 2017.\nI am considering ‘wife’, ‘husband’, ‘son’ and ‘daughter’ (and their plural and possessive forms) as these are the most immediate family members.\nI am also including ‘marriage’ and ‘divorce’ since these are likely sources of distress or conflict that could push somebody to submit a question to the column.\n\nwife&lt;-sum(grepl(\"\\\\bwives\\\\b|\\\\bwife'?s?'?\\\\b\", data$question_only))\nhusband&lt;-sum(grepl(\"\\\\bhusband'?s?'?\\\\b\", data$question_only))\nson&lt;-sum(grepl(\"\\\\bson'?s?'?\\\\b\", data$question_only))\ndaughter&lt;-sum(grepl(\"\\\\bdaughter'?s?'?\\\\b\", data$question_only))\nmarriage&lt;-sum(grepl(\"\\\\bmarriage'?s?'?\\\\b|\\\\bmarried\\\\b\", data$question_only))\ndivorce&lt;-sum(grepl(\"\\\\bdivorces?d?\", data$question_only))\nfamily_data&lt;-data.frame(Wife=wife, Husband=husband, Son=son, Daughter=daughter, Marriage=marriage, Divorce=divorce)\n\nThe table below shows the number of questions asked to Abby from 1985 to 2017 that included each term (including its plural or possessive forms).\n\nprint(family_data)\n\n  Wife Husband  Son Daughter Marriage Divorce\n1 2775    5124 2320     2750     4887    1534\n\n\nIt is very interesting to note that ‘wife’, ‘son’ and ‘daughter’ all appeared a similar number of times, while ‘husband’ was by far the most frequent, appearing almost twice as many times as the other terms (5124 times - this is approximately one quarter of the 20034 questions!) ‘Marriage’ was also very common, starring in 4887 questions.\n\n\nNumber of Questions Through the Years\nFor some more exploratory data analysis, I wanted to observe how the number of questions per year changed over time:\n\ny_questions&lt;-data|&gt; group_by(year)|&gt; summarize(n_questions=n())\n\n\nggplot(y_questions, aes(x=year, y=n_questions)) +\n  geom_point(color='pink') +\n  theme_dark() +\n  labs(x=\"Year\",\n       y=\"Number of Questions\",\n       title=\"'Dear Abby' Questions Over The Years\",\n       subtitle=\"From 1985 to 2017\")\n\n\n\n\n\n\n\n\nIt seems like 1985 had an abnormally high number of questions. After this, the number of questions decreased steadily until 1999, when the number of questions per year began to increase again. It did so until 2016, and experienced a steep drop in 2017.\n\n\nQuestion Length\nThen, I was curious about the length of the questions in the dataset:\n\ndata|&gt;\n  select(year, question_only)|&gt;\n  mutate(size=str_length(question_only))|&gt;\n  arrange(desc(size))|&gt;\n  slice_head(n=10)\n\n# A tibble: 10 × 3\n    year question_only                                                      size\n   &lt;dbl&gt; &lt;chr&gt;                                                             &lt;int&gt;\n 1  1988 \"media-wise, this is going to be a tough campaign for barbara bu… 13889\n 2  1995 \"on one page of prison life magazine, an inmate describes his fa… 13411\n 3  1990 \"this story has been entered on the data base in 2 parts. this i… 13225\n 4  1988 \"kitty carlisle hart has spent her life in the\\nspotlight. panel… 11690\n 5  1992 \"this story has been entered on the data base in 3 parts. this i… 11550\n 6  1995 \"arthur donald delacy would have--by all rights, should have--tu… 11087\n 7  1989 \"with his arms stuffed in his pockets to the elbow, the cartooni… 10716\n 8  1995 \"while charles schulz sits at his table, pen in hand, drawing th… 10275\n 9  1994 \"i am so fed up with store employees treating me and my friend l…  8867\n10  1986 \"denise salvaggio's dream is to do for bugs what walt disney did…  8514\n\n\nThe longest question occurred in 1998, with a size of 13889 characters - and it was about Barbara Bush!\nWhat about the shortest question?\n\nq_length&lt;-data|&gt;\n  select(year, question_only)|&gt;\n  mutate(size=str_length(question_only))|&gt;\n  arrange(size)|&gt;\n  slice_head(n=10)\nprint(q_length)\n\n# A tibble: 10 × 3\n    year question_only                                            size\n   &lt;dbl&gt; &lt;chr&gt;                                                   &lt;int&gt;\n 1  1996 \"q: were you alone or by yourself?\"                        33\n 2  1996 \"q: so you were gone until you returned?\"                  39\n 3  1990 \"how would you define old age? getting there\"              43\n 4  1996 \"q: how long have you been a french canadian?\"             44\n 5  1986 \"what would you give a man who has everything? freda\"      51\n 6  1990 \"how would you define success? philosophy major, ucla\"     52\n 7  1996 \"q: the youngest son, the 20-year-old, how old is he?\"     52\n 8  2008 \"it's apathy ... but, who cares? -- malcolm in miami\\n\"    52\n 9  1996 \"q: do you have any children or anything of that kind?\"    53\n10  2010 \"how do you mend a broken heart? -- tears on my pillow\"    53\n\n\nIt occurred in 1996, with the rather ominous: “q: were you alone or by yourself?”. In 2010, someone asked a very hard question, but kept it incredibly succint at 53 characters: “how do you mend a broken heart?– tears on my pillow”.\n\n\n47 - Chirp Chirp!\nAt Pomona College, the number 47 holds special meaning: it is said to be a number that frequently occurs in natural settings. With this in mind, I wanted to determine how often the number 47 appears in this dataset.\n\ndata&lt;-data|&gt;\n  mutate(chirp=str_detect(question_only, \"47\"))\nsum(data$chirp)\n\n[1] 227\n\n\nThe number 47 appears 227 times in the dataset, out of 20034 letters written to Abby."
  },
  {
    "objectID": "miniproject2.html#distinguishing-common-themes-using-word-count",
    "href": "miniproject2.html#distinguishing-common-themes-using-word-count",
    "title": "Text Data Analysis Using Regular Expressions",
    "section": "Distinguishing Common Themes Using Word Count",
    "text": "Distinguishing Common Themes Using Word Count\nI would like to determine which words are the most commonly occurring across all the questions to see if any common themes can be extracted.\nFirst, stop words need to be removed - these are common words in the English language that don’t have much meaning. The stop_words dataframe from the tidytext library contains 3 lexicons of stop words.\nWe should also remove the words ‘dear’ and ‘abby’ (and its variants) since those will be at the start of almost every letter.\n\ndear_abby &lt;- c(\"dear\", \"abby\", \"abby's\", \"abbys\")\n\n\nWord Cloud - What Concerned Americans Most Between 1985 and 2017?\nBelow is a word cloud displayed the 150 most common words from 32 years ‘Dear Abby’ letters (excluding stop words):\n\nwc &lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^\\\\D+$\"))|&gt;\n  filter(!word %in% dear_abby)|&gt;\n  group_by(word)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(desc(freq))\n\n\nlibrary(wordcloud2)\nlibrary(wordcloud)\nlibrary(RColorBrewer)\nwordcloud(words = wc$word, freq = wc$freq, min.freq = 600, max.words=150, random.order=FALSE, rot.per=0, colors=brewer.pal(6, \"Set2\"), scale = c(5, 1))\n\n\n\n\n\n\n\n\nWe can see from both this word cloud and the earlier analysis done on family words that Americans are very concerned by family matters and family members: ‘husband’, ‘children’, ‘kids’, ‘mom’, ‘mother’, ‘married’, ‘family’, ‘wife’, ‘son’, ‘father’, ‘parents’, ‘sister’, to name a few.\nFrom the word cloud, we can also distinguish the most common emotions: ‘hurt’, ‘afraid’, ‘enjoy’, ‘concerned’, ‘upset’.\n\n\nMost Common Word by Year\n\nwords_by_year&lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^\\\\D+$\"))|&gt;\n  filter(!word %in% dear_abby)|&gt;\n  group_by(word, year)|&gt;\n  summarize(freq=n())|&gt;\n  ungroup()|&gt;\n  group_by(year)|&gt;\n  slice_max(n = 1, order_by=freq)|&gt;\n  arrange(year)\n\n\nggplot(words_by_year, aes(x=year, y=freq)) +\n  geom_point(aes(shape=word, color=word), size=2) +\n  labs(title=\"Most Common Word in 'Dear Abby' Letters\",\n       subtitle= \"By Year, 1985 to 2017\",\n       x=\"Year\",\n       y=\"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nHusband Letters Over Time\nSince ‘husband’ was the most common word between 1985 and 2017, it would be interesting to see how its frequency changed over time:\n\nhusband_year&lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^husband'?s?$\"))|&gt;\n  group_by(year)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(year)\n\n\nggplot(husband_year, aes(x=year, y=freq)) +\n  geom_point(color=\"darkgreen\", shape=18, size=2.5) +\n  theme_minimal() +\n  labs(title=\"Frequency of 'Husband' in 'Dear Abby' Letters\",\n       subtitle= \"1985 to 2017\",\n       x=\"Year\",\n       y=\"Number of Occurrences\")\n\n\n\n\n\n\n\n\n\n\nElection Letters Over Time\nLastly, I was interested in determining whether the word ‘election’ (and its plural form) was a common concern. I also wanted to see if it became more frequent during US Presidential Election years.\n\nelections_year&lt;- data|&gt;\n  unnest_tokens(output=word, input=question_only, token=\"words\")|&gt;\n  anti_join(stop_words)|&gt;\n  filter(str_detect(word, \"^elections?$\"))|&gt;\n  group_by(year)|&gt;\n  summarize(freq=n())|&gt;\n  arrange(year)\n\n\nggplot(elections_year, aes(x=year, y=freq)) +\n  geom_bar(stat=\"identity\", aes(fill = ifelse(year %in% c(1988, 1992, 1996, 2000, 2004, 2008, 2012, 2016), \"Yes\", \"No\"))) +\n  scale_fill_manual(values = c(\"Yes\" = \"navyblue\", \"No\" = \"gray\")) +\n  theme_minimal() +\n  labs(title=\"'Dear Abby' Election Letters\",\n       subtitle=\"Years With and Without US Presidential Elections\",\n       y=\"Year\",\n       x=\"Frequency\",\n       fill=\"Election Year?\")\n\n\n\n\n\n\n\n\nThe word ‘election’ and its plural form ‘elections’ does not seem to be a popular concern, never appearing more than 4 times in one year. However, the plot above shows that these words were more common during US Presidential Election years than during years without a Presidential Election, with the exception of 1994."
  },
  {
    "objectID": "project4.html",
    "href": "project4.html",
    "title": "SQL",
    "section": "",
    "text": "For this project, I will be using SQL to query Smith College’s Wideband Acoustic Immittance (WAI) Database, which contains WAI ear measurements from a multitude of scientific publications. The WAI Database is available at https://www.science.smith.edu/wai-database/. My goal is twofold. First, I aim to use SQL and ggplot to replicate Figure 1 from Susan Voss’ 2019 study, “An online wideband acoustic immittance (WAI) database and corresponding website” (Voss 2019). Second, I aim to produce a plot showing race differences in frequency versus mean absorption for one specific study in the WAI database (done in 2010 study by Voss et al.).\n\n\n\nlibrary(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nThis displays all of the tables that are in the WAI database. Measurements and Subjects will be of particular interest for what I aim to do.\nMeasurements contains information about each specific study, notably the frequency and absorbance.\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nSubjects contains information about the participants in each study, including their age, sex, race and ethnicity.\n\nSELECT *\nFROM Subjects\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nAbur_2014\n1\n7\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n3\n8\n19\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 5 not included do to acoustic leak\n\n\nAbur_2014\n4\n7\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n6\n8\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n7\n5\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\n\n\n\nPI_Info contains information about the authors, year, journal and title of each study in the database. This will be useful for including the primary author and date of publication in the replicated figure legend.\n\n\n\n\nSELECT p.Identifier, p.Year, p.AuthorsShortList, Frequency,\nLOG10(Frequency) AS log_frequency, AVG(Absorbance) AS mean_absorbance,\nCOUNT(DISTINCT SubjectNumber, Ear) AS ear_u,\nCONCAT(AuthorsShortList, \" (\" , year, \") \", \"N=\", COUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS legend\nFROM PI_Info AS p\nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier\nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\") AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument, Frequency;\n\n\nplot |&gt;\nggplot(aes(x = Frequency, y = mean_absorbance,\n  color = legend,\n  group = legend)) +\n  geom_line() +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Mean absorbance from each publication in WAI database\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Study, No of individual ears, Equipment\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n  theme(legend.text = element_text(size = 4),\n        legend.title = element_text(size = 6),\n        legend.position = c(0.01, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.4, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nThis plot above is a replication of the figure in Voss’ 2019 study (Voss 2019). It displays frequency versus mean absorbance measurements for the 12 studies included in the WAI database (as of July 2019). The authors, number of unique ears and equipment used for each study is indicated in the legend in the top left.\n\n\n\n\nSELECT s.Race, m.Frequency, AVG(m.Absorbance) AS mean_absorbance \nFROM Subjects AS s \nRIGHT JOIN Measurements AS m ON s.SubjectNumber = m.SubjectNumber\nWHERE m.Identifier = \"Voss_2010\" AND m.Frequency &gt; 200 AND m.Frequency &lt; 8000\nGROUP BY s.Race, m.Frequency;\n\n\nhead(groups_graph)\n\n              Race Frequency mean_absorbance\n1 African American  210.9375      0.06189557\n2 African American  234.3750      0.03557816\n3 African American  257.8125      0.06311273\n4 African American  281.2500      0.05740714\n5 African American  304.6875      0.06617098\n6 African American  328.1250      0.07566153\n\n\n\ngroups_graph |&gt;\n  ggplot(aes(x = Frequency, y = mean_absorbance)) +\n           geom_line(aes(color = Race)) +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Differences in Mean Absorbance by Race\",\n       subtitle = \"Voss et al., 2010\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Race\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n    theme(legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8),\n        legend.position = c(0.05, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.6, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nI chose to look at differences in mean absorbance for each frequency band by race in one specific study conducted by Voss et al. in 2010. I chose this study because of its large number of participants (1984), which should help to make trends in the data more clear. Each line represents a different race included in the study. It can be seen from the graph that mean absorbance follows very similar trends for White, Caucasian and Asian participants. Meanwhile, the mean absorbances for African American, Black and Mixed participants are noticeably different from those of the other races. Notably, African American participants tend to have a higher mean absorbance for the same frequencies than the other races included in the study, especially for frequencies between 1000 Hz and 5000 Hz. Mixed participants tend to have lower mean absorbances than the other races in the study for frequencies between 2000 Hz and 4000 Hz.\n\n\n\nIn this project I used SQL queries to obtain the necessary information from the relevant tables in the WAI database (Subjects, Measurements and PI_Info). These SQL queries enabled me to save the data as R objects, from which I could produce graphs using ggplot. I was able to replicate Figure 1 from Voss’ 2019 study (Voss 2019), and I also produced a graph that showed clear race differences in mean absorbance for Voss et al.’s 2010 study."
  },
  {
    "objectID": "project4.html#data-familiarization",
    "href": "project4.html#data-familiarization",
    "title": "SQL",
    "section": "",
    "text": "library(mdsr)\nlibrary(dbplyr)\nlibrary(DBI)\n\n\nlibrary(RMariaDB)\nlibrary(tidyverse)\n\ncon_wai &lt;- dbConnect(\n  MariaDB(), host = \"scidb.smith.edu\",\n  user = \"waiuser\", password = \"smith_waiDB\", \n  dbname = \"wai\"\n\n)\n\nMeasurements &lt;- tbl(con_wai, \"Measurements\")\nPI_Info &lt;- tbl(con_wai, \"PI_Info\")\nSubjects &lt;- tbl(con_wai, \"Subjects\")\n# collect(Measurements)\n\n\nSHOW TABLES;\n\n\n7 records\n\n\nTables_in_wai\n\n\n\n\nCodebook\n\n\nMeasurements\n\n\nMeasurements_pre2020\n\n\nPI_Info\n\n\nPI_Info_OLD\n\n\nSubjects\n\n\nSubjects_pre2020\n\n\n\n\n\nThis displays all of the tables that are in the WAI database. Measurements and Subjects will be of particular interest for what I aim to do.\nMeasurements contains information about each specific study, notably the frequency and absorbance.\n\nSELECT *\nFROM Measurements\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSession\nEar\nInstrument\nAge\nAgeCategory\nEarStatus\nTPP\nAreaCanal\nPressureCanal\nSweepDirection\nFrequency\nAbsorbance\nZmag\nZang\n\n\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n210.938\n0.0333379\n113780000\n-0.233504\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n234.375\n0.0315705\n103585000\n-0.235778\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n257.812\n0.0405751\n92951696\n-0.233482\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n281.250\n0.0438399\n86058000\n-0.233421\n\n\nAbur_2014\n1\n1\nLeft\nHearID\n20\nAdult\nNormal\n-5\n4.42e-05\n0\nAmbient\n304.688\n0.0486400\n79492800\n-0.232931\n\n\n\n\n\nSubjects contains information about the participants in each study, including their age, sex, race and ethnicity.\n\nSELECT *\nFROM Subjects\nLIMIT 0, 5;\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIdentifier\nSubjectNumber\nSessionTotal\nAgeFirstMeasurement\nAgeCategoryFirstMeasurement\nSex\nRace\nEthnicity\nLeftEarStatusFirstMeasurement\nRightEarStatusFirstMeasurement\nSubjectNotes\n\n\n\n\nAbur_2014\n1\n7\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n3\n8\n19\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\nSession 5 not included do to acoustic leak\n\n\nAbur_2014\n4\n7\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n6\n8\n21\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\nAbur_2014\n7\n5\n20\nAdult\nFemale\nUnknown\nUnknown\nNormal\nNormal\n\n\n\n\n\n\nPI_Info contains information about the authors, year, journal and title of each study in the database. This will be useful for including the primary author and date of publication in the replicated figure legend."
  },
  {
    "objectID": "project4.html#replicating-the-figure-from-voss-2019",
    "href": "project4.html#replicating-the-figure-from-voss-2019",
    "title": "SQL",
    "section": "",
    "text": "SELECT p.Identifier, p.Year, p.AuthorsShortList, Frequency,\nLOG10(Frequency) AS log_frequency, AVG(Absorbance) AS mean_absorbance,\nCOUNT(DISTINCT SubjectNumber, Ear) AS ear_u,\nCONCAT(AuthorsShortList, \" (\" , year, \") \", \"N=\", COUNT(DISTINCT SubjectNumber, Ear), \"; \", Instrument) AS legend\nFROM PI_Info AS p\nLEFT JOIN Measurements AS m ON m.Identifier = p.Identifier\nWHERE p.Identifier IN (\"Abur_2014\", \"Feeney_2017\", \"Groon_2015\" ,\"Lewis_2015\", \"Liu_2008\", \"Rosowski_2012\", \"Shahnaz_2006\", \"Shaver_2013\" , \"Sun_2016\", \"Voss_1994\", \"Voss_2010\", \"Werner_2010\") AND Frequency &gt; 200 AND Frequency &lt; 8000\nGROUP BY Identifier, Instrument, Frequency;\n\n\nplot |&gt;\nggplot(aes(x = Frequency, y = mean_absorbance,\n  color = legend,\n  group = legend)) +\n  geom_line() +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Mean absorbance from each publication in WAI database\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Study, No of individual ears, Equipment\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n  theme(legend.text = element_text(size = 4),\n        legend.title = element_text(size = 6),\n        legend.position = c(0.01, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.4, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nThis plot above is a replication of the figure in Voss’ 2019 study (Voss 2019). It displays frequency versus mean absorbance measurements for the 12 studies included in the WAI database (as of July 2019). The authors, number of unique ears and equipment used for each study is indicated in the legend in the top left."
  },
  {
    "objectID": "project4.html#race-differences-in-mean-absorbance",
    "href": "project4.html#race-differences-in-mean-absorbance",
    "title": "SQL",
    "section": "",
    "text": "SELECT s.Race, m.Frequency, AVG(m.Absorbance) AS mean_absorbance \nFROM Subjects AS s \nRIGHT JOIN Measurements AS m ON s.SubjectNumber = m.SubjectNumber\nWHERE m.Identifier = \"Voss_2010\" AND m.Frequency &gt; 200 AND m.Frequency &lt; 8000\nGROUP BY s.Race, m.Frequency;\n\n\nhead(groups_graph)\n\n              Race Frequency mean_absorbance\n1 African American  210.9375      0.06189557\n2 African American  234.3750      0.03557816\n3 African American  257.8125      0.06311273\n4 African American  281.2500      0.05740714\n5 African American  304.6875      0.06617098\n6 African American  328.1250      0.07566153\n\n\n\ngroups_graph |&gt;\n  ggplot(aes(x = Frequency, y = mean_absorbance)) +\n           geom_line(aes(color = Race)) +\n  scale_x_log10(breaks = c(200, 400, 600, 800, 1000, 2000, 4000, 6000, 8000)) +\n  labs(title = \"Differences in Mean Absorbance by Race\",\n       subtitle = \"Voss et al., 2010\",\n      x = \"Frequency (Hz)\",\n      y = \"Mean Absorbance\",\n      color = \"Race\") +\n  coord_cartesian(xlim = c(200, 8000), ylim = c(0, 1)) +\n  theme_minimal() +\n    theme(legend.text = element_text(size = 6),\n        legend.title = element_text(size = 8),\n        legend.position = c(0.05, 0.98),\n        legend.justification = c(\"left\", \"top\"),\n        legend.key.size = unit(0.6, \"lines\"),\n    legend.background = element_rect(fill = \"white\", color = \"black\")\n  )\n\n\n\n\n\n\n\n\nI chose to look at differences in mean absorbance for each frequency band by race in one specific study conducted by Voss et al. in 2010. I chose this study because of its large number of participants (1984), which should help to make trends in the data more clear. Each line represents a different race included in the study. It can be seen from the graph that mean absorbance follows very similar trends for White, Caucasian and Asian participants. Meanwhile, the mean absorbances for African American, Black and Mixed participants are noticeably different from those of the other races. Notably, African American participants tend to have a higher mean absorbance for the same frequencies than the other races included in the study, especially for frequencies between 1000 Hz and 5000 Hz. Mixed participants tend to have lower mean absorbances than the other races in the study for frequencies between 2000 Hz and 4000 Hz."
  },
  {
    "objectID": "project4.html#conclusion",
    "href": "project4.html#conclusion",
    "title": "SQL",
    "section": "",
    "text": "In this project I used SQL queries to obtain the necessary information from the relevant tables in the WAI database (Subjects, Measurements and PI_Info). These SQL queries enabled me to save the data as R objects, from which I could produce graphs using ggplot. I was able to replicate Figure 1 from Voss’ 2019 study (Voss 2019), and I also produced a graph that showed clear race differences in mean absorbance for Voss et al.’s 2010 study."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Charlotte Imbert",
    "section": "",
    "text": "Welcome! I’m Charlotte Imbert, an M.S. Statistics student at Stanford University. Before that, I obtained my Bachelor’s degree in Neuroscience with a minor in Mathematics at Pomona College.\nI build practical machine learning and analytics that turn high-dimensional data into decisions. I’m especially excited about technical roles in tech and biotech, where scalable systems and human-centered products meet.\nWith IBM at the Wimbledon Championships (2022 & 2023), I analyzed tennis points in real-time and helped create the live stats millions of fans saw and engaged with. That pace taught me precision under pressure, rigorous attention to detail, and clear communication with operations teams. This experience motivated the tennis statistical analyses that can be found on my website.\nAfter 18 years of competitive tennis and a career-ending nerve injury, I pivoted from playing to understanding health, performance, and recovery. At Stanford’s Systems Neuroscience & Pain Lab, I had the opportunity to build predictive ML models on clinical datasets. This involved wrangling messy data, rigorous validation, and translating findings for pain clinicians.\nWhat sets my work apart is my focus on statistical soundness and real-world applicability. I pay close attention to model interpretability, not just accuracy. I choose models suitable for the problem’s structure, validate out of sample, and explain uncertainty in plain language so clinicians and product teams can act with the right level of confidence. I build end-to-end in Python and R, and I’m proficient in SQL, MATLAB, and Excel. My neuroscience research experience has made me especially sensitive to the importance of quality data and thorough experimental design (garbage in, garbage out!).\nI’m fluent in four languages (English, French, Spanish, Portuguese) and love collaborative teams. This site is my portfolio. If you’re in big tech or biotech and exploring data/ML work, I’d love to connect.\n(Actively seeking Summer 2026 data science/PM internships.)\nIn my spare time, you can find me playing pickleball tournaments, hiking in the California nature, performing stand-up comedy, playing trivia, or hanging out with friends.\n\n\n\n\nWorking at Wimbledon with IBM in 2022 (left) and 2023 (right)\n\n \n\nRepresenting Pomona College at DataFest @ UCLA in 2023!\n\n \n\nWinning 2nd Prize at Harvey Mudd College’s DSSI Datathon (left) and presenting my analysis of MLB bat speed and swing length at the CSAS conference at Yale in 2025 (right)"
  },
  {
    "objectID": "atp.html",
    "href": "atp.html",
    "title": "How to Win in Tennis",
    "section": "",
    "text": "With an off-season only a month long and a loaded tournament schedule year-round, professional tennis players on the Association of Tennis Professionals (ATP) Tour have limited practice time to improve their game. Understanding how particular match statistics influence a player’s winning percentage can help ‘to determine the match statistics to which coaches should attend in an effort to mould elite professional players’ during this scarce practice time (Reid, McMurtrie, and Crespo 2010). A greater winning percentage leads to gains in ATP world ranking points, which results in direct entry to more prestigious tournaments and consequently increases financial gains through prize money. This is of great importance because of the high annual cost of travel, coaching and equipment, which ‘can total between $121,000 and $197,000 USD’ for male professionals (Reid et al. 2014). Only an elite few earn lucrative sponsorship deals, meaning that the majority of tennis professionals must rely on prize money to finance both their life on tour and their life after retirement from professional tennis.\nPrevious analysis of ATP Tour data from the 2007 season carried out by Reid et al. (2010) observed the relationship between 14 tennis match statistics and a player’s world ranking (Reid, McMurtrie, and Crespo 2010). They found that second serve return points won and second serve points won were the most significant predictors of a player’s world ranking. The second serve statistic provides insight on a player’s ability to win points that generally wouldn’t be won: servers take less risk when hitting second serves, as an error here automatically loses them the point. Thus, these are often weaker and less well-placed than first serves, giving the returner a better chance of returning the serve and of winning the point, hence the value of looking at the second serve return statistic as well. In addition to these serve and return statistics, this study will consider players’ winners to unforced error ratio, which is a good indicator of both tactical aggressiveness and consistency, and the percentage of points played at the net, which is another measure of aggressiveness. These statistics were not considered in Reid et al.’s (2010) study (Reid, McMurtrie, and Crespo 2010). This analysis will also differ from the aforementioned study in that the response variable observed will be winning percentage rather than world ranking. Additionally, player age and height will be included in our selection of explanatory variables.\nPrevious literature suggests the presence of a height advantage in men’s professional tennis: in Ovaska et al.’s (2014) analysis of ATP match data from 2000 to 2009, it was found that ‘a higher ranked taller player was 3.1% more likely to win, other things constant,’ than a shorter player (Ovaska, Sumell, and Sumell 2014). However, this ‘advantage diminishes with increasing height’ as at very tall heights players tend to lose mobility (Ovaska, Sumell, and Sumell 2014). The same study found that ‘higher ranked older players have a disadvantage over their younger opponents’ and that ‘the larger the disparity in age, the larger the probability that the older player actually loses’ (Ovaska, Sumell, and Sumell 2014). Of course, a coach and player cannot work on changing height or age, but this previous literature suggests that these factors should be taken into account when attempting to predict a male professional tennis player’s winning percentage.\nFor this study, we will analyze singles match data from the 2022 ATP Tour to attempt to build a model predicting a player’s match winning percentage in a season based on age, height and 4 match statistics. We aim to show the effect of changes in each match statistic on a touring professional’s winning percentage at a certain height and age, providing guidance for coaches and players on which tactical aspects to work on. The data and variables we will consider will be covered in the methods section, model fitting and inference will be found in the data analysis section, and the conclusion will summarize findings and comment on possible model improvements."
  },
  {
    "objectID": "atp.html#introduction",
    "href": "atp.html#introduction",
    "title": "How to Win in Tennis",
    "section": "",
    "text": "With an off-season only a month long and a loaded tournament schedule year-round, professional tennis players on the Association of Tennis Professionals (ATP) Tour have limited practice time to improve their game. Understanding how particular match statistics influence a player’s winning percentage can help ‘to determine the match statistics to which coaches should attend in an effort to mould elite professional players’ during this scarce practice time (Reid, McMurtrie, and Crespo 2010). A greater winning percentage leads to gains in ATP world ranking points, which results in direct entry to more prestigious tournaments and consequently increases financial gains through prize money. This is of great importance because of the high annual cost of travel, coaching and equipment, which ‘can total between $121,000 and $197,000 USD’ for male professionals (Reid et al. 2014). Only an elite few earn lucrative sponsorship deals, meaning that the majority of tennis professionals must rely on prize money to finance both their life on tour and their life after retirement from professional tennis.\nPrevious analysis of ATP Tour data from the 2007 season carried out by Reid et al. (2010) observed the relationship between 14 tennis match statistics and a player’s world ranking (Reid, McMurtrie, and Crespo 2010). They found that second serve return points won and second serve points won were the most significant predictors of a player’s world ranking. The second serve statistic provides insight on a player’s ability to win points that generally wouldn’t be won: servers take less risk when hitting second serves, as an error here automatically loses them the point. Thus, these are often weaker and less well-placed than first serves, giving the returner a better chance of returning the serve and of winning the point, hence the value of looking at the second serve return statistic as well. In addition to these serve and return statistics, this study will consider players’ winners to unforced error ratio, which is a good indicator of both tactical aggressiveness and consistency, and the percentage of points played at the net, which is another measure of aggressiveness. These statistics were not considered in Reid et al.’s (2010) study (Reid, McMurtrie, and Crespo 2010). This analysis will also differ from the aforementioned study in that the response variable observed will be winning percentage rather than world ranking. Additionally, player age and height will be included in our selection of explanatory variables.\nPrevious literature suggests the presence of a height advantage in men’s professional tennis: in Ovaska et al.’s (2014) analysis of ATP match data from 2000 to 2009, it was found that ‘a higher ranked taller player was 3.1% more likely to win, other things constant,’ than a shorter player (Ovaska, Sumell, and Sumell 2014). However, this ‘advantage diminishes with increasing height’ as at very tall heights players tend to lose mobility (Ovaska, Sumell, and Sumell 2014). The same study found that ‘higher ranked older players have a disadvantage over their younger opponents’ and that ‘the larger the disparity in age, the larger the probability that the older player actually loses’ (Ovaska, Sumell, and Sumell 2014). Of course, a coach and player cannot work on changing height or age, but this previous literature suggests that these factors should be taken into account when attempting to predict a male professional tennis player’s winning percentage.\nFor this study, we will analyze singles match data from the 2022 ATP Tour to attempt to build a model predicting a player’s match winning percentage in a season based on age, height and 4 match statistics. We aim to show the effect of changes in each match statistic on a touring professional’s winning percentage at a certain height and age, providing guidance for coaches and players on which tactical aspects to work on. The data and variables we will consider will be covered in the methods section, model fitting and inference will be found in the data analysis section, and the conclusion will summarize findings and comment on possible model improvements."
  },
  {
    "objectID": "atp.html#methods",
    "href": "atp.html#methods",
    "title": "How to Win in Tennis",
    "section": "Methods",
    "text": "Methods\nMatch statistics and biographical player information used in this analysis were obtained from the Ultimate Tennis Statistics website 1. The statistics found on this website are gathered from the open-source data repository by Jeff Sackmann 2, storing in-match statistics recorded from every ATP match since 1991. The response variable we will try to predict is winning percentage, which is the percentage of matches won by a player during the tennis season (January to November). We consider winning percentage rather than number of matches won to account for the different numbers of matches played across a season by different players due to factors such as injury and tournament schedule. The predictor variables that we will consider are player height in cm (‘height’), player age in years as of the end of 2022 (‘age’), the percentage of second serve points won when returning (‘return’), the percentage of second serve points won when serving (‘serve’), the ratio of winners to unforced errors (‘ratio’), and the percentage of points played at the net (‘net’).\nConsidered in this study were the 100 male professional tennis players with the highest number of singles matches played on the ATP Tour in 2022. This was done because a statistic is more likely to be representative of a player’s skill in that tactical area over a large number of matches played in a variety of conditions, which can affect outcomes. As this is a ranking, the data was not randomly sampled. All of the players in this analysis regularly competed on the ATP Tour during the 2022 season and as such the results will only apply to touring professional male tennis players. Additionally, this analysis will only consider singles tennis, which is tactically very different to doubles. Thus, conclusions drawn from singles data cannot be directly applied to doubles. It must also be noted that the observations in this sample are not independent, as each win must necessarily be accompanied by a loss. However, this dependence is negligible over the course of a season. Additionally, as the sample only contains 100 players out of the population of touring professionals, each player’s winning percentage is influenced by certain wins and losses against other players not sampled, which diminishes this dependence. Each player’s winning percentage is a function of the number of matches he competes in, which varies from player to player. Thus, unless the winning percentage were 100 (which does not occur in the sample), knowing the winning percentage for one player does not reveal information about the winning percentage for another player on the tour. As the dependence of the observations is negligible here, we can conclude that the independence assumption is reasonably fulfilled and proceed with fitting a multiple regression."
  },
  {
    "objectID": "atp.html#data-analysis",
    "href": "atp.html#data-analysis",
    "title": "How to Win in Tennis",
    "section": "Data Analysis",
    "text": "Data Analysis\nThe variables included in the final model were height, serve, ratio, and return cubed, with the slope coefficients and associated p-values displayed in Table 1 and a y-intercept of -207.8. All four predictor variables were significant at the 0.001 level. The adjusted R-squared for the model was 0.6037, with a residual standard error of 8.678. The model can be represented by the following equation:\n Table 1. The predictor variables included in the model along with their slope coefficients and corresponding p-values.\nThis model was arrived at via the following procedure. With assistance from the Box-Cox function in R, it was deemed that no transformation of the winning percentage response variable was necessary. Each of the 6 predictor variables were then input into the Box-Cox function to verify if any transformations were required. The suggested transformations were applied and added to the data frame only if the resulting variable remained interpretable. The transformed variables that were added to the data frame were the reciprocal of age, the square root of ratio, the natural logarithm of net, and return cubed. The updated data frame was then subjected to both stepwise AIC and relaxed LASSO. Both methods suggested adding height, serve, ratio, and return cubed to the model. The addition of other variables was suggested, but these fits yielded insignificant p-values and had negligible effects on adjusted R-squared values. For the sake of model simplicity and interpretability, it was decided to include height, serve, ratio, and return3 in the final model, with 𝛽 values and p-values shown in Table 1.\nThe appropriateness of this multiple linear regression model can be evaluated via the residual plots shown in Figure 1. It can be seen in Figure 1a) that the data follows a linear trend as the average residual locally appears to be around 0 for these fitted values, with the exception of one outlier. Therefore, the linear assumption is satisfactorily fulfilled. Figure 1c) indicates that homoskedasticity is also satisfied as the studentized residuals are structured roughly in a band either side of the red line. This line is not perfectly straight, but from the general distribution of the studentized residuals we can reasonably say that the homoskedasticity assumption is not violated. The Q-Q plot in Figure 1b) shows that the normality of residuals assumption is satisfied. The outlier at observation 4, shown in the residuals vs. leverage plot in Figure 1d), has a high leverage but a Cook’s Distance less than 0.5. This low Cook’s Distance implies that this is a benevolent leverage point that is not changing the response surface in a concerning way. Its omission is therefore not necessary: in fact, a benevolent high-leverage point is desirable as points far away from the rest of the data lead to more accurate slope estimates for the variables in the model.\nThe final model tells us that 0.6037 of the variance in a touring professional’s winning percentage can be explained by a combination of height, the cubed percentage of second serve points won when returning, the ratio of winners to unforced errors, and the percentage of second serve points won when serving. How can this model be used for informative purposes? It tells us that, with the other three predictor variables constant, a unit increase in ratio will increase winning percentage by 2.523, while a unit increase in the percentage of second serve points won when serving will increase winning percentage by 1.398. For every cm in height advantage a player has over his opponents, with all other variables constant, his winning percentage is expected to increase by 0.7023. As the return variable was subjected to a cubic transformation, it is perhaps simpler to see its effect on the winning percentage through an example. If a player of height 180cm, serve 47% and ratio 1.25 increased his return value from 50% to 51%, his winning percentage would increase from 31.06% to 33.73%, based on the model’s fitted values. If instead this same player’s return value remained at 50%, with serve and height constant, his ratio of winners to unforced errors would have to increase from 1.25 to 2.31 to reach this improved winning percentage. With height, return and ratio constant, this same player would need to improve his serve value from 47% to 48.91% to improve his winning percentage from 31.06% to 33.73%. Lastly, with serve, return and ratio constant and equal to the initial values specified above, the model tells us that a height increase from 180 cm to 183.8 cm would improve the winning percentage to 33.73%.\nConfidence intervals can be used to estimate the value of the mean winning percentage for a touring professional with specific predictor variable values. In this case, our confidence interval tells us that we are 95% confident that the mean winning percentage for a touring professional 188cm tall with 50.96% of second serve points won when serving, 51.22% of second serve points won when returning and a 1.073 ratio of winners to unforced errors is expected to lie between 50.29% and 55.64%. Prediction intervals were also obtained for each observation in the sample. These estimate the value of the winning percentage for a specific new observation with defined values for each predictor variable. Constructing a prediction interval using the same predictor variable values as above (188cm, 50.96%, 51.22%, 1.073) tells us that we are 95% confident that the winning percentage for the next touring professional with these predictor values will lie between 35.53% and 70.40%. This interval is much larger than the confidence interval as it accounts for the many sources of noise specific to that tennis player that cannot be explained by the variables in our model.\n Figure 1. Residual plots resulting from the fitted multiple linear regression model."
  },
  {
    "objectID": "atp.html#conclusion",
    "href": "atp.html#conclusion",
    "title": "How to Win in Tennis",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, four variables were found to be significant in explaining variability in match winning percentage. These were height in cm, the ratio of winners to unforced errors, the percentage of second serve points won when serving, and the cubed percentage of second serve points won when returning. A multiple linear regression model was constructed using these four explanatory variables. Evaluation of residual plots concluded that the model was suitably appropriate as it did not violate any of the linear regression assumptions. The adjusted R-squared value of 0.6037 for this fit indicates that 60.37% of the variability in match winning percentage can be explained by the variables included in the model. The remaining variability can be attributed to hundreds of additional in-match statistics that could be considered, as well as the factor of luck, especially as tennis is a sport where the outcome of a handful of key points can determine the result of a match.\nThis follows Reid et al.’s (2010) findings that the percentage of second serves won when both serving and returning are significant in predicting world ranking, as a good ranking is a direct consequence of high winning percentage (Reid, McMurtrie, and Crespo 2010). Additionally, these slope coefficients provide support for Ovaska et al.’s (2014) claim that there is a height advantage in men’s tennis but, contrary to these authors’ findings, does not suggest that this advantage subsists past a certain height (Ovaska, Sumell, and Sumell 2014). If a greater number of very tall touring professionals were sampled, this claim could be better evaluated. The fact that there are very few touring professionals above 200 cm could suggest that the height advantage decreases past a certain point, otherwise professional tennis, like basketball for example, would have higher numbers of very tall players. This work also revealed that a player’s ratio of winners to unforced errors is an important predictor, which had not been tested in either of the aforementioned studies. However, neither the percentage of points played at the net nor age proved significant in predicting winning percentage. This is contrary to Ovaska et al.’s (2014) finding that younger professional tennis players have an advantage over their older counterparts (Ovaska, Sumell, and Sumell 2014).\nImportantly for real-world applications, each of the four predictor variables has a positive slope coefficient, meaning that increasing any of these values has the ability to increase winning percentage. While height is not something that players and coaches can change, knowing that height affects winning percentage suggests that shorter players must seek increased gains in tactical areas compared to their taller counterparts. It is therefore up to the coach and player to decide which of these three significant match statistics to focus on, depending on the player’s current skill set, in order to improve match winning percentage and consequently ranking and prize money earnings.\nAnalysis of outliers also provides important information about the limitations of the model. An outlier with a lower winning percentage than expected was detected. This data point corresponds to Novak Djokovic, one of the most successful male players of all time, winning 22 Grand Slam singles titles 3. The fitted value for a player with his explanatory response values was 95.78%, while his actual winning percentage was 87.23%. This very high fitted value results from his exceptional match statistics over a season. Out of a year’s worth of matches, it is expected for even the most elite players to occasionally underperform and lose, without their match statistics over the year being significantly affected. An added problem that comes from attempting to predict percentages is that there is a floor at 0% and a ceiling at 100% which the model does not take into account. For example, some of the prediction interval values were above 100%, which of course is not possible. This model could be improved by analyzing a response variable that, while not a percentage, can still provide information on a player’s success, for example number of wins per month, or total ranking points won in the year."
  },
  {
    "objectID": "atp.html#footnotes",
    "href": "atp.html#footnotes",
    "title": "How to Win in Tennis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAvailable at Ultimate Tennis Statistics, accessed 3/29/23.↩︎\nAvailable at GitHub - Jeff Sackmann Tennis ATP, accessed 3/29/23. winning 22 Grand Slam singles titles.↩︎\nRetrieved from Sporting News, accessed 3/31/23.↩︎"
  },
  {
    "objectID": "shakespeare.html",
    "href": "shakespeare.html",
    "title": "Shakespeare",
    "section": "",
    "text": "Data was accessed from the TidyTuesday GitHub repositroy at https://github.com/rfordatascience/tidytuesday/tree/master/data/2024/2024-09-17 . The dataset is from shakespeare.mit.edu (via github.com/nrennie/shakespeare), which contains all of William Shakespeare’s plays and poems. Some of the code below is taken from Deepali Kank on GitHub, https://github.com/deepdk/TidyTuesday2024/tree/main/2024/week_38 .\n\nlibrary(tidyverse)\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/romeo_juliet.csv')\n\n\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(devtools)\nlibrary(ggwordcloud)\nlibrary(png)\nlibrary(svglite)"
  },
  {
    "objectID": "shakespeare.html#the-data",
    "href": "shakespeare.html#the-data",
    "title": "Shakespeare",
    "section": "",
    "text": "Data was accessed from the TidyTuesday GitHub repositroy at https://github.com/rfordatascience/tidytuesday/tree/master/data/2024/2024-09-17 . The dataset is from shakespeare.mit.edu (via github.com/nrennie/shakespeare), which contains all of William Shakespeare’s plays and poems. Some of the code below is taken from Deepali Kank on GitHub, https://github.com/deepdk/TidyTuesday2024/tree/main/2024/week_38 .\n\nlibrary(tidyverse)\nhamlet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/hamlet.csv')\nmacbeth &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/macbeth.csv')\nromeo_juliet &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-09-17/romeo_juliet.csv')\n\n\nlibrary(scales)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(devtools)\nlibrary(ggwordcloud)\nlibrary(png)\nlibrary(svglite)"
  },
  {
    "objectID": "shakespeare.html#romeo-juliet",
    "href": "shakespeare.html#romeo-juliet",
    "title": "Shakespeare",
    "section": "Romeo & Juliet",
    "text": "Romeo & Juliet\nFor this TidyTuesday project, I am interested in using text mining to observe the most commonly spoken words by Romeo and Juliet in the famous Shakespeare play. The first step is to filter for only those lines spoken by Romeo or Juliet.\n\nromeo_juliet&lt;-romeo_juliet |&gt;\n  filter(character %in% c(\"Romeo\", \"Juliet\"))\n\nFor any text mining analysis, stop words (the most common words in a language) need to be removed. Bearing in mind that this is dialogue from the 16th century, modern stop words need to be converted in their 16th century equivalents.\n\ncustom_stop_words &lt;- c(\"thou\", \"thy\", \"thee\", \"thine\", \"art\", \"hast\", \"doth\", \"dost\", \"ere\", \"o\",\"hath\")\n\nNext, we use a tidy pipeline to separate each line of dialogue into individual words, remove both modern and 16th century stop words, remove the possessive ‘s’, and remove strings that are entirely numbers using regular expression matching.\n\nword_counts &lt;- romeo_juliet |&gt;\n  unnest_tokens(word, dialogue) |&gt;\n  anti_join(stop_words) %&gt;%\n  filter(!str_detect(word, \"^[0-9]+$\")) |&gt;\n  filter(!word %in% custom_stop_words) |&gt;\n  mutate(word=stringr::str_replace(word, \"'s\", \"\")) |&gt;\n  count(character, word, sort = TRUE)\n\nThen, word counts for each word are computed individually for Romeo and Juliet.\n\njuliet &lt;- word_counts |&gt; \n  filter(character == \"Juliet\")\nhead(juliet)\n\n# A tibble: 6 × 3\n  character word       n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Juliet    romeo     41\n2 Juliet    love      35\n3 Juliet    night     30\n4 Juliet    nurse     20\n5 Juliet    sweet     16\n6 Juliet    tybalt    14\n\n\n\nromeo &lt;- word_counts |&gt; \n  filter(character == \"Romeo\")\nhead(romeo)\n\n# A tibble: 6 × 3\n  character word       n\n  &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt;\n1 Romeo     love      52\n2 Romeo     death     22\n3 Romeo     juliet    17\n4 Romeo     fair      15\n5 Romeo     night     15\n6 Romeo     mine      14\n\n\nWe can then display these word counts via word clouds, in the shape of William Shakespeare himself!\n\np1_ro &lt;- ggplot(\n  romeo,\n  aes(\n    label = word, size = n,color = n\n  )\n) +\n  geom_text_wordcloud_area(\n    mask = readPNG(\"/Users/charlotteimbert/Desktop/git/cameraperture.github.io/AlphaShakespeare.png\"),\n    rm_outside = TRUE\n  ) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  scale_color_gradient(low = \"#FF69B4\", high = \"#C41E3A\")\n\nprint(p1_ro)\n\n\n\n\n\n\n\n\nThe word cloud above represents Romeo’s most common words, while the one below displays Juliet’s most common words.\n\np1_ju &lt;- ggplot(\n  juliet,\n  aes(\n    label = word, size = n,color = n\n  )\n) +\n  geom_text_wordcloud_area(\n    mask = readPNG(\"/Users/charlotteimbert/Desktop/git/cameraperture.github.io/AlphaShakespeare.png\"),\n    rm_outside = TRUE\n  ) +\n  scale_size_area(max_size = 20) +\n  theme_minimal() +\n  scale_color_gradient(low = \"#FF69B4\", high = \"#C41E3A\")\n\nprint(p1_ju)\n\n\n\n\n\n\n\n\nJuliet’s most common word was Romeo, but Romeo’s most common word was not Juliet - instead, it was ‘love’."
  },
  {
    "objectID": "wta.html",
    "href": "wta.html",
    "title": "Advantage, Height: Do Taller Female Professional Tennis Players Serve Better?",
    "section": "",
    "text": "It is common knowledge that height provides an advantage in certain sports, most notably basketball. When it comes to professional tennis however, height is less commonly associated with success. Should height be considered an important factor in tennis-playing ability? Here we will seek to determine if a height advantage exists in professional women’s tennis, specifically in terms of the serve. This is arguably the most important shot in the sport as it initiates each point. It seems logical for a taller player to serve faster as she would be able to strike the ball down from a greater height than a shorter player, resulting in more power. Cernosek and Vaverka (2016) attribute height to a ‘biomechanical advantage that helps [players] obtain a higher serve speed: taller players with longer limbs will achieve a higher peripheral racket speed at the moment of hitting the ball than shorter players with the same angular velocity of upper extremity segments’ (Cernosek and Vaverka 2016). Additionally, Bonato et al. (2015) found height to be ‘the main anthropometric determinant of serve speed in professional tennis players’ (Bonato et al. 2015). However, this study only looked at 8 professional players, all of which were male. Cernosek and Vaverka’s (2016) analysis was more substantial as it looked at both male and female tennis players who competed at all four Grand Slam tournaments between 2008 and 2012. Their analysis found statistically significant positive correlations between body height and first serve speed (Cernosek and Vaverka 2016). Similarly, Gallagher et al.’s (2021) analysis of more recent data from the 2013 to 2019 Grand Slam tournaments found ‘a strong positive correlation between the median percent of aces per match and height’ for both male and female players (Gallagher, Frisoli, and Luby 2021). An ace is a serve that is not returned by the opponent. Thus, an ace count indicates the number of points a player wins using only their serve. With an increased likelihood of serving faster, taller players may hit more aces as a fast serve ‘puts the opponent under stress and may hinder its return’ (Martin 2018). However, Martin (2018) further states that serve speed is not the only factor in determining ace count as spin and placement are also important (Martin 2018). As explained by Cernosek and Vaverka (2016), taller players ‘will achieve higher hand-racket angular momentum’ which may give them an advantage in terms of spin and placement (Cernosek and Vaverka 2016).\nThis analysis will contribute to the discussion on the effect of height on the professional tennis serve by providing insight on some of the most recent data available: we will examine the female players comprising the Women’s Tennis Association (WTA) top 100 rankings as of December 7th, 2022 (WTA.com 2022). Moreover, unlike previous studies, this analysis will not be limited to the Grand Slam tournaments and will instead take into account all tournaments in 2022 that contributed to professional ranking points. We will try to determine if a relationship exists between player height and the number of aces hit per match. Taking the aforementioned biomechanical factors into account, we will attempt to see if taller female professional players are better servers and perhaps better tennis players, the ability to win service games being crucial to winning matches."
  },
  {
    "objectID": "wta.html#introduction",
    "href": "wta.html#introduction",
    "title": "Advantage, Height: Do Taller Female Professional Tennis Players Serve Better?",
    "section": "",
    "text": "It is common knowledge that height provides an advantage in certain sports, most notably basketball. When it comes to professional tennis however, height is less commonly associated with success. Should height be considered an important factor in tennis-playing ability? Here we will seek to determine if a height advantage exists in professional women’s tennis, specifically in terms of the serve. This is arguably the most important shot in the sport as it initiates each point. It seems logical for a taller player to serve faster as she would be able to strike the ball down from a greater height than a shorter player, resulting in more power. Cernosek and Vaverka (2016) attribute height to a ‘biomechanical advantage that helps [players] obtain a higher serve speed: taller players with longer limbs will achieve a higher peripheral racket speed at the moment of hitting the ball than shorter players with the same angular velocity of upper extremity segments’ (Cernosek and Vaverka 2016). Additionally, Bonato et al. (2015) found height to be ‘the main anthropometric determinant of serve speed in professional tennis players’ (Bonato et al. 2015). However, this study only looked at 8 professional players, all of which were male. Cernosek and Vaverka’s (2016) analysis was more substantial as it looked at both male and female tennis players who competed at all four Grand Slam tournaments between 2008 and 2012. Their analysis found statistically significant positive correlations between body height and first serve speed (Cernosek and Vaverka 2016). Similarly, Gallagher et al.’s (2021) analysis of more recent data from the 2013 to 2019 Grand Slam tournaments found ‘a strong positive correlation between the median percent of aces per match and height’ for both male and female players (Gallagher, Frisoli, and Luby 2021). An ace is a serve that is not returned by the opponent. Thus, an ace count indicates the number of points a player wins using only their serve. With an increased likelihood of serving faster, taller players may hit more aces as a fast serve ‘puts the opponent under stress and may hinder its return’ (Martin 2018). However, Martin (2018) further states that serve speed is not the only factor in determining ace count as spin and placement are also important (Martin 2018). As explained by Cernosek and Vaverka (2016), taller players ‘will achieve higher hand-racket angular momentum’ which may give them an advantage in terms of spin and placement (Cernosek and Vaverka 2016).\nThis analysis will contribute to the discussion on the effect of height on the professional tennis serve by providing insight on some of the most recent data available: we will examine the female players comprising the Women’s Tennis Association (WTA) top 100 rankings as of December 7th, 2022 (WTA.com 2022). Moreover, unlike previous studies, this analysis will not be limited to the Grand Slam tournaments and will instead take into account all tournaments in 2022 that contributed to professional ranking points. We will try to determine if a relationship exists between player height and the number of aces hit per match. Taking the aforementioned biomechanical factors into account, we will attempt to see if taller female professional players are better servers and perhaps better tennis players, the ability to win service games being crucial to winning matches."
  },
  {
    "objectID": "wta.html#methods",
    "href": "wta.html#methods",
    "title": "Advantage, Height: Do Taller Female Professional Tennis Players Serve Better?",
    "section": "Methods",
    "text": "Methods\nThe sample used in this analysis consists of the players ranked in the WTA top 100 rankings as of December 7th, 2022. The two variables we are interested in are player height (in cm) and the number of aces hit per match. Data regarding height, number of aces and number of matches played in 2022 was obtained from the ‘Stats’ section on the WTA website. The matches included in these statistics consist of all of the professional matches played on the WTA circuit in the 2022 calendar year in addition to those played at the Grand Slam tournaments, managed by the International Tennis Federation.  As the WTA does not provide an aces per match statistic, this had to be calculated separately in Excel instead. It was decided to observe aces hit per match rather than the total number of aces hit in the year as the former provides a better indication of a player’s serving ability. This is because there is great variability between the numbers of matches played by each player in a season due to factors such as injury, illness, ranking, finances, sponsorships, nationality or pregnancy. It is logical to assume that a player who has played more matches in the year will have a higher total ace count, and consequently an aces per match statistic is more representative.\nAs the data being considered is a rankings list, it was not randomly sampled. It was decided to observe the top 100 players because this threshold typically represents the most elite performers in the sport. Consequently, lack of playing experience or differing skill level are likely to play a smaller part in the results observed, allowing us to focus more on the effect of height. The results of this analysis will therefore apply to professionals rather than amateurs. Additionally, observing 100 players allows for a balance between the time-consuming nature of manual data input into Excel and the maximization of sample size. Data for two players, ranked 72 and 92, could not be used as the WTA database did not include these players’ heights. Their heights could not be found on other online databases. In order to keep a sample size of 100, data from the players ranked 101 and 102 was used instead."
  },
  {
    "objectID": "wta.html#data-analysis",
    "href": "wta.html#data-analysis",
    "title": "Advantage, Height: Do Taller Female Professional Tennis Players Serve Better?",
    "section": "Data Analysis",
    "text": "Data Analysis\nAs the two variables in question are numeric, any potential relationship between them can be identified using a regression. An initial least-squares regression of the form y = β0 + β1x  was carried out, with intercept β0 and slope β1. In a situation where no relationship exists between height and the number of aces per match, the best-fitting line is expected to be flat. Therefore, H0 : β1 = 0 and HA : β1 ≠ 0. The least-squares regression yielded a line with β1 = 0.15611 (Figure 1), and the t-statistic for this slope value was 7.105 with a p-value of 0.0000000001953. While this very low p-value seems to indicate that there exists a relationship between height and the number of aces per match, the suitability of this linear fit must be checked before any conclusions can be drawn. \nThe residual plot shown in Figure 2 allows us to assess the suitability of the linear model. The three underlying assumptions for using a linear regression are that the residuals display linearity, homoskedasticity and normality. The linearity assumption seems fulfilled here as there appears to be a roughly even number of points scattered either side of the residual = 0 line. However, the residuals show heteroskedasticity as variability increases with height, notably from approximately 170 cm onwards (Figure 2). Log-transforming both variables in the linear model (Figure 4) yields a more appropriate fit which is more successful at fulfilling not only the linearity assumption but also the homoskedasticity and normality assumptions. The residual plot in Figure 5 shows points scattered either side of the residual = 0 line with much more constant variance than the one resulting from the linear model (Figure 2). The Normal Q-Q plot in Figure 6 shows that the residuals resulting from the log-transformed linear model are closer to normality than those resulting from the linear model (Figure 3). It is not a concern that these log-transformed residuals do not show perfect normality as the sample size of 100 is large, meaning that the central limit theorem is in place. The log-transformed fit is therefore a more appropriate model for the data as it satisfies these three assumptions much more suitably than the linear model. \nThis log-transformed regression takes the form log(y) = β0 + β1log(x), while the hypotheses remain H0 : β1 = 0 and HA : β1 ≠ 0. The regression yields a β1 value of 11.366, with a t-statistic of 6.479 and p-value of 0.00000000371. We once again observe an extremely low p-value, much lower than  α = 0.05. This p-value indicates that it is virtually impossible to have observed such structured data in a world where no relationship exists between the two variables. Therefore, we can reject H0 : β1 = 0. Back-transforming the x and y variables of this log-transformed model informs us that the number of aces per match will increase on average by 0.16 for each 1 cm increase in height. With a multiple R-squared value of 0.2999, we can infer that there exists a moderate positive correlation between player height and the number of aces per match. This value indicates that just under a third of the variation in the number of aces per match can be explained by height. The residual standard error of 0.5626 refers to the variability in the number of aces per match that can be explained by factors other than height. Three outliers (observations 59, 62 and 76, Figure 4) exemplify the influence of these other factors as these players have aces per match counts contradictory to what would be expected for their heights."
  },
  {
    "objectID": "wta.html#conclusion",
    "href": "wta.html#conclusion",
    "title": "Advantage, Height: Do Taller Female Professional Tennis Players Serve Better?",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, a moderate positive correlation was found between a professional tennis player’s height and her number of aces hit per match. While not perfect by any means, a log-transformed linear regression model was deemed a suitable approximation for the data, predicting a mean increase of about 0.16 aces per match for each 1 cm increase in height. This model also suggests that just under a third of the variability in the number of aces per match can be explained by height in female professional tennis. The rest of the variability can be explained by factors including technique, strength, strategy, court surface (fast versus slow court) and weather conditions. These other factors can be used to provide explanations for the three outliers observed in the data: three taller players underperformed according to the log-transformed model, hitting fewer aces per match than expected. These players may have played a higher proportion of their matches on clay courts, where the conditions are slower and therefore less conducive to aces. Alternatively, they may have used less effective technique or strategy than other players. It is possible that using a larger sample size would have resulted in a stronger correlation between height and aces per match. This analysis only considers the top 100 female players in the world. If instead the WTA’s top 1000 players were considered, a clearer pattern between the two variables may emerge. \nWhile the data supports the suggestion that a taller professional female tennis player is likely to hit more aces per match than her shorter colleagues, it must be noted that this does not necessarily make her a better server. While ace counts give us a good indication of a player’s serving ability, they are not the best metric. Many times, a good serve can force the opponent to miss her return while still hitting the ball with the racket. This means that these serves, while effective, are not classified as aces as the returner’s racket touched the ball. Additionally, this analysis did not consider double faults or first serve percentages. A player may serve in a risky way that results in a high ace count counterbalanced by a high double-fault count or a lower first serve percentage. This would result in an overall serving ability that is not reflected in a simple aces per match count. A statistic such as a player’s percentage of service games won would be a more suitable reflection of her serving ability. Carrying out a similar analysis looking at this statistic instead of the number of aces per match would consequently be more representative of serving and overall tennis ability."
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "158 - Homework 4",
    "section": "",
    "text": "1.\nWe have the model \\(y_i=\\beta_0+\\beta_1 x_i + \\epsilon_i\\). We can compute the residuals \\(e_i=y_i-\\hat{\\beta}_0 - \\hat{\\beta}_1 x_i\\) which approximate the \\(\\epsilon_i\\) if the model fits (otherwise it’s not clear what we’re computing). Recalling that the variance of the errors is the “average squared deviation from the mean”, we would love to compute \\[\\frac{1}{n}\\sum (\\epsilon_i - \\mu_{\\epsilon})^2 = \\frac{1}{n}\\sum \\epsilon_i^2\\]\nBut since the \\(\\epsilon\\) are unknowable, we instead compute \\[\\frac{1}{n-2}\\sum (e_i - \\bar{e})^2 = \\frac{1}{n-2}\\sum e_i^2 = \\frac{1}{n-2}SSE = MSE\\]\nThe \\(n-2\\) is to fix the fact that SSE is smaller than \\(\\sum \\epsilon_i^2\\) by virtue of doing optimization.\nIf the model fits, \\(MSE\\) is estimating \\(\\sigma^2_{\\epsilon}\\) (unbiasedly).\nA more general model is that \\(y_i = \\mu_{x_i} + \\epsilon_i\\), and this model certainly fits (though requires estimating a lot more parameters than 2, one for each unique value of \\(x\\) in the data set. Also, it is unable to predict observations at \\(x\\) variables you haven’t seen yet). We can once again compute residuals in this model, \\(e_i^*=y_i-\\bar{y}_{x_i}\\). We are going to need replications at some of the \\(x\\) values to keep these from all being 0. Suppose that there are \\(c\\) unique values of \\(x\\) in the data set, with \\(c &lt; n\\).\nFrom these, we can compute \\[\\frac{1}{n-c}\\sum (e_i^* - \\bar{e^*})^2 = \\frac{1}{n-c}\\sum e_i^{*2} = \\frac{1}{n-c}SSPE = MSPE\\] which is certainly a (less stable) estimate of \\(\\sigma^2\\). (Less stable because it’s based on less information due to having to estimate more parameters, but we know the model fits). PE stands for “pure error”.\nWhat shows up in the formal test of linearity is \\(MSLF\\) (lack of fit) = \\(\\frac{1}{c-2} (SSE - SSPE)\\). Aruge using the above facts and some simple algebra that \\(MSLF\\) is also estimating \\(\\sigma^2_{\\epsilon}\\).\nAs a consequence, argue that the F statistic used should be about 1 if the null (linear model is appropriate) is true.\n\n\n2.\nThe sakai site contains a data set consisting of brain mass to body mass for 27(ish) species. Fit a model predicting brain mass based on body mass, and additionally include a confidence interval and prediction interval (to be introduced on Tuesday, you’ll be able to steal code from those lecture notes). Fully justify and interpret your model, writing in complete sentences."
  },
  {
    "objectID": "swift.html",
    "href": "swift.html",
    "title": "Taylor Swift and Kacey Musgraves Lyrical Analysis",
    "section": "",
    "text": "The release of her 10th album Midnights and the announcement of her long-awaited Eras Tour made Taylor Swift one of the most streamed and discussed music artists in 2022. Through the years Swift has experimented with different genres, and although she is now predominantly known as a pop artist, her career originated from country music. Sloan (2021) points out a ‘key inflection point in [Swift’s] career’; although she ‘had been moving towards a pop sound since 2012’s Red’, Swift ‘repositioned herself from a country star to a pop star with the release of 2014’s 1989’ (Sloan 2021). Kacey Musgraves is another example of an artist with strong country roots who transitioned to pop during her career. While her genre transition was not as radical as Swift’s, Musgraves was classified by the Recording Academy as a pop artist with the release of her 2021 album star-crossed, which featured in the pop vocal album category for the 2022 Grammy Awards. Although she has not achieved the level of fame Swift has, Musgraves has ‘continually earn[ed] critical praise and even scored two Grammy awards for her debut album’ (Smith 2016). Despite both artists’ success, country music remains a male-dominated space, with Swift being ‘the last female artist to have her first two singles reach the Top 10 on Billboard’s Hot Country Songs chart […] all the way back in 2007’ (Smith 2016). This may have influenced both Musgraves’ and Swift’s decision to shift genres from country to pop during their careers.\nOne of the key differences between country and pop music is songwriting structure, specifically in the chorus. As Neal (2007) explains, since the 1980s country music has been built on the ‘Time-Shift paradigm’, which features the ‘presence of a verse-chorus song form […] a sequence of verses that center on the multi-generational life-cycle and attention to family in chronologically distinct episodes […] a reinterpretation of the chorus’s text and meaning in each iteration’ (Neal 2007).  Sloan (2021) writes that in such music ‘the lyrical material of the chorus shifts over each appearance’ while ‘most pop songs repeat the exact same chorus for each appearance of the section’ (Sloan 2021). While Swift’s first three albums used the Time-Shift paradigm, her fourth album Red left it behind: Sloan (2021) cites I Knew You Were Trouble (Red), with its chorus repeating identically each time, as exemplifying ‘the stasis of pop form’ (Sloan 2021). Thus, due to these structural differences, we would expect country songs to be less repetitive than pop songs, and consequently to have a higher number of unique words - a tally where repeating words are counted only once.\nIn this analysis, we will look at four albums from each artist. These albums are, chronologically, Swift’s Fearless, Speak Now, Red and 1989, and Musgraves’ Same Trailer, Different Park, Pageant Material, Golden Hour, and star-crossed. Each artist’s earlier two albums will be classified as country while the latter two will be classified as pop. We will seek to determine whether Swift’s and Musgraves’ country songs have a higher unique word count than their pop songs. Additionally, we will determine whether there are differences in unique word count between the two artists. It is without a doubt that Swift has been the more successful artist, with her second album Fearless amassing more Spotify streams than all four of Musgraves’ albums combined (Spotify 2023). While there are many factors influencing artist success and personal music preference, there is a view that ‘some songs are structurally and aesthetically “better” than others and are therefore preferred’ (Boyle, Hosterman, and Ramsey 1981). Could songwriting structure, more specifically unique word count, contribute to this aesthetic superiority that leads to success?"
  },
  {
    "objectID": "swift.html#introduction",
    "href": "swift.html#introduction",
    "title": "Taylor Swift and Kacey Musgraves Lyrical Analysis",
    "section": "",
    "text": "The release of her 10th album Midnights and the announcement of her long-awaited Eras Tour made Taylor Swift one of the most streamed and discussed music artists in 2022. Through the years Swift has experimented with different genres, and although she is now predominantly known as a pop artist, her career originated from country music. Sloan (2021) points out a ‘key inflection point in [Swift’s] career’; although she ‘had been moving towards a pop sound since 2012’s Red’, Swift ‘repositioned herself from a country star to a pop star with the release of 2014’s 1989’ (Sloan 2021). Kacey Musgraves is another example of an artist with strong country roots who transitioned to pop during her career. While her genre transition was not as radical as Swift’s, Musgraves was classified by the Recording Academy as a pop artist with the release of her 2021 album star-crossed, which featured in the pop vocal album category for the 2022 Grammy Awards. Although she has not achieved the level of fame Swift has, Musgraves has ‘continually earn[ed] critical praise and even scored two Grammy awards for her debut album’ (Smith 2016). Despite both artists’ success, country music remains a male-dominated space, with Swift being ‘the last female artist to have her first two singles reach the Top 10 on Billboard’s Hot Country Songs chart […] all the way back in 2007’ (Smith 2016). This may have influenced both Musgraves’ and Swift’s decision to shift genres from country to pop during their careers.\nOne of the key differences between country and pop music is songwriting structure, specifically in the chorus. As Neal (2007) explains, since the 1980s country music has been built on the ‘Time-Shift paradigm’, which features the ‘presence of a verse-chorus song form […] a sequence of verses that center on the multi-generational life-cycle and attention to family in chronologically distinct episodes […] a reinterpretation of the chorus’s text and meaning in each iteration’ (Neal 2007).  Sloan (2021) writes that in such music ‘the lyrical material of the chorus shifts over each appearance’ while ‘most pop songs repeat the exact same chorus for each appearance of the section’ (Sloan 2021). While Swift’s first three albums used the Time-Shift paradigm, her fourth album Red left it behind: Sloan (2021) cites I Knew You Were Trouble (Red), with its chorus repeating identically each time, as exemplifying ‘the stasis of pop form’ (Sloan 2021). Thus, due to these structural differences, we would expect country songs to be less repetitive than pop songs, and consequently to have a higher number of unique words - a tally where repeating words are counted only once.\nIn this analysis, we will look at four albums from each artist. These albums are, chronologically, Swift’s Fearless, Speak Now, Red and 1989, and Musgraves’ Same Trailer, Different Park, Pageant Material, Golden Hour, and star-crossed. Each artist’s earlier two albums will be classified as country while the latter two will be classified as pop. We will seek to determine whether Swift’s and Musgraves’ country songs have a higher unique word count than their pop songs. Additionally, we will determine whether there are differences in unique word count between the two artists. It is without a doubt that Swift has been the more successful artist, with her second album Fearless amassing more Spotify streams than all four of Musgraves’ albums combined (Spotify 2023). While there are many factors influencing artist success and personal music preference, there is a view that ‘some songs are structurally and aesthetically “better” than others and are therefore preferred’ (Boyle, Hosterman, and Ramsey 1981). Could songwriting structure, more specifically unique word count, contribute to this aesthetic superiority that leads to success?"
  },
  {
    "objectID": "swift.html#methods",
    "href": "swift.html#methods",
    "title": "Taylor Swift and Kacey Musgraves Lyrical Analysis",
    "section": "Methods",
    "text": "Methods\nThe albums for this analysis were chosen such that two country and two pop albums were considered for each artist. Only four albums were considered for each artist as this is the number of records that Musgraves has released, excluding Christmas albums. Additionally, Swift’s 2006 self-titled album was not considered. If this album had been included, either Fearless or Speak Now would have had to be omitted, despite being newer albums, in order to keep a balanced design with two country and two pop albums. This would result in a dataset consisting of albums that were not released consecutively, which would not provide the most accurate representation of Swift’s career trajectory. Thus, Swift’s self-titled album was omitted from this analysis. For the purposes of this study, the albums will be numbered as follows: Fearless and Same Trailer, Different Park (album 1, country), Speak Now and Pageant Material (album 2, country), Red and Golden Hour (album 3, pop), and finally 1989 and star-crossed (album 4, pop).\nTen songs were randomly selected from each album using the ‘sample.int’ function in RStudio. This resulted in a total of 80 songs, with each one constituting an observation in the dataset. Lyrics were then obtained from azlyrics.com and the unique word count for each song was determined using the ‘length’, ‘strsplit’ and ‘unique’ functions in RStudio (AZLyrics 2023). Unique word count requires that repeating words be counted only once, which gives an indication of how varied a song’s lyrics are. The two factors for this two-way ANOVA are artist and album number, with the response variable being a song’s unique word count. \nA key assumption for ANOVA is that the errors are independent and identically distributed. Although the songs for each album were randomly sampled (identical distribution), and knowing the unique word count of one song does not give an indication of the unique word count of another song (independence), it must be noted that we are making the assumption that the songs in each album constitute an infinite population. This is not the case, as there is a finite number of songs in each album and a finite number of songs written by an artist at each stage of her career. This consequently limits the real-world applications of the inference that this dataset allows us to do."
  },
  {
    "objectID": "swift.html#data-analysis",
    "href": "swift.html#data-analysis",
    "title": "Taylor Swift and Kacey Musgraves Lyrical Analysis",
    "section": "Data Analysis",
    "text": "Data Analysis\nFirst, it was necessary to determine whether an additive or an interactive model would be suitable for the dataset. Visualization of an interaction plot (Figure 1) indicated the possibility of an interaction between the factors of artist and album number as the lines were not parallel. As a result, an ANOVA for an interactive model was fitted. While artist and album number were individually significant, the interaction term was insignificant (p-value 0.64867), and so this model was abandoned. The lines in the interaction plot are based on means for the sample rather than on the true population means, which explains why they did not appear parallel despite no significant interaction being present.\nConsequently, an additive model consisting of the two factors of artist and album number was fitted instead. Verification of the residual plots for this model showed that the ANOVA assumptions were satisfied. The relatively straight horizontal red lines in the Residuals vs. Fitted plot (Figure 2a) and in the Scale-Location plot (Figure 2c) show that homoskedasticity holds. Inspection of the Normal Q-Q plot (Figure 2b) suggests normality of the residuals, but this was checked more rigorously using a Shapiro-Wilk test. This yielded a p-value of 0.2784, indicating that it is unlikely that the residuals were not normally distributed. As the assumptions were reasonably fulfilled, the additive model was deemed suitable and was used for inference.\nThe ANOVA table for this model showed  the presence of a clear artist effect, with Taylor Swift using a higher number of unique words than Kacey Musgraves, regardless of the album (p-value 1.684 x 10-11). There was also an album effect, although more subtle than the artist effect (p-value 0.01642). The interaction plot (Figure 1) showed that both artists used more unique words on average in their first and second albums compared to their third and fourth albums, while their second albums contained the highest average unique word count out of the four. On average, Swift had more unique words in her fourth album than her third, while the opposite was true for Musgraves. Further tests were carried out to obtain greater insight on differences in unique word count between albums and between artists. These tests consisted of six album pairwise differences, one artist pairwise difference, and one contrast between country and pop albums.To account for multiple testing, the overall ⍺ level of 0.05 was divided among these tests using Bonferroni’s Inequality. An adjusted ⍺ value of 0.03 was allocated to the album pairwise differences as there were six of them, while an ⍺ value of 0.01 was allocated to both the artist pairwise difference and the genre contrast. \nThe pairwise differences were determined via a Tukey multiple comparisons of means test, with a family-wise confidence level of 97% for album number and 99% for artist. With regards to album number, the only significant difference was between albums 3 and 2 (adjusted p-value 0.0271). The 97% confidence interval for this difference had a lower bound of -44.8 and an upper bound of -0.3. This means that we are 97% confident that a song on Golden Hour or Red will have between 44.8 and 0.3 fewer unique words than a song on Pageant Material or Speak Now. Additionally, the artist pairwise difference was very significant (adjusted p-value 0). The 99% confidence interval for this difference had a lower bound of -58.78 and upper bound of -29.37, meaning that we are 99% confident that a Kacey Musgraves song from any of her non-Christmas albums will have between 29.37 and 58.78 fewer unique words than a song from Taylor Swift’s Fearless, Speak Now, Red or 1989. \n\nthe t-multiplier obtained using qt(0.005, 75) in RStudio. 75 refers to the degrees of freedom and 0.005 results from halving the adjusted significance level of 0.01 to cover both tails of the distribution. This yielded an upper bound for the confidence interval of 31.38 and a lower bound of 1.97. While this interval is large, it does not include 0. This allows us to say with 99% confidence that, indeed, Musgraves’ and Swift’s country albums differ significantly from their pop albums in terms of unique word count."
  },
  {
    "objectID": "swift.html#conclusion",
    "href": "swift.html#conclusion",
    "title": "Taylor Swift and Kacey Musgraves Lyrical Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThe purpose of this study was twofold: to determine whether there is a difference in unique word count between the country and pop albums of Taylor Swift and Kacey Musgraves, and whether there is a difference in unique word count between these two artists. After an initial interactive ANOVA model was deemed unsuitable, an additive model was fitted. Evaluation of residual plots considered the additive model appropriate as it did not violate ANOVA assumptions. This additive fit showed both an artist and an album number effect. Tukey pairwise comparisons of means were carried out to obtain greater insight on these factor effects. It was found that, in the albums considered, Swift’s songs had an extremely significantly higher unique word count than Musgraves’ songs, with the 99% confidence interval for this difference ranging between 29.37 and 58.78 unique words. This artist effect speaks to the different songwriting styles of the two artists and indicates that Swift uses more variety in her lyrics than Musgraves. While there are many aspects that contribute to an artist’s success, this could provide some insight as to why Swift’s music has been more critically acclaimed (12 Grammys compared to Musgraves’ 6) and more streamed by the public (84 million monthly listeners on Spotify compared to Musgraves’ 6.5 million) (The Recording Academy 2023) (Spotify 2023).\nThe only significant album pairwise difference identified was between albums 3 and 2, which corresponds to the difference between each artist’s first pop record and her last country record. This difference was negative, indicating that both artists used fewer unique words in their first pop record than in their last country record. Perhaps more insightful is the contrast calculated between country and pop albums across both artists. It was found that, with 99% confidence, a song on country records Fearless, Speak Now, Same Trailer, Different Park or Pageant Material, will have between 31.38 and 1.97 more unique words than a song on pop records Red, 1989, Golden Hour or star-crossed. This genre difference in unique words agrees with Neal’s (2007) description of the Time-Shift paradigm and is aptly summarized by Sloan (2021): ‘As she moved out of the world of country, Swift shifted her formal language away from the transformative narratives of country to the static emotional tension of pop’ (Neal 2007) (Sloan 2021). \nA limitation of this study is the assumption that the observations came from an infinite population, which is not the case. Each album only contains between 12 and 20 songs, and each artist wrote a finite number of songs at each stage in her career, including the ones that did not make it onto an album. The nature of the data therefore limits the scope of the inference that we are able to carry out. Additionally, there is variability in song length that could influence the number of unique words in a song regardless of its genre. Thus, this analysis could be improved by changing the response variable to unique words per minute. Lastly, our analysis could also be improved by increasing the number of songs analyzed, but this is simply not realistic as Musgraves only has 4 albums, each with 15 songs or less. Of course, only two artists were considered in this analysis. It would be of interest to consider other artists who followed a similar trajectory from country to pop, consequently adding more levels to the artist factor. These could include male artists, allowing us to determine whether there is a sex difference in the number of unique words used in songs."
  },
  {
    "objectID": "kaggle_gmsc.html",
    "href": "kaggle_gmsc.html",
    "title": "Kaggle Competition - Give Me Some Credit",
    "section": "",
    "text": "Competition link: https://www.kaggle.com/c/GiveMeSomeCredit"
  },
  {
    "objectID": "kaggle_gmsc.html#kaggle-competition---give-me-some-credit",
    "href": "kaggle_gmsc.html#kaggle-competition---give-me-some-credit",
    "title": "Kaggle Competition - Give Me Some Credit",
    "section": "",
    "text": "Competition link: https://www.kaggle.com/c/GiveMeSomeCredit"
  }
]